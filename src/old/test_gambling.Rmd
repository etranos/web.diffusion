---
title: "diffusion, websites with 1 pc"
date: "`r format(Sys.time(), '%d %B, %Y, %H:%M')`"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_float: true
knit: (function(inputFile, encoding) {
    rmarkdown::render(inputFile, encoding = encoding, output_dir = "../output")
  })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rprojroot)
library(urltools)

options(scipen=10000)

# This is the project path
path <- find_rstudio_root_file()
```

all

```{r}
data.folder <- "/hdd/internet_archive/archive/data/"
data.path.all <- paste0(data.folder, "all.csv")
all <- read_csv(data.path.all)

g <- all %>% filter(year==2010) %>% 
  filter(grepl('gambl', url)) #

#g %>% filter(year==2010)

#all %>% filter(grepl("^www.", host))
```
## hyperlinks

zgrep "gambleaware" host-linkage.tsv.gz | sort > gambleaware.tsv

```{r}
links.path <- "/home/nw19521/Downloads/gambleaware.tsv"

l <- read_delim(links.path, delim = "|", col_names = F) %>% 
  separate(X3, c('j', 'links'), sep = "\t") %>% 
  rename(year = X1,
         i = X2)
```
gamble is obtained by:
zgrep "gamble" host-linkage.tsv.gz | sort > gamble.tsv

It contains the hyperlink between domains that include the word "gamble".
There are 47795 obs (`dim(l)`) and 46293 unique hyperlinks (`length(unique(l$ij))`).

**Key message** the links are not repeated over time, only 47795 - 46293 = 1202.
2004 is the link-farm problem. 2008-2010 the number of links drops.

After i do the i join, there 935378 obs and 35751 postcode NA.

```{r}
links.path <- "/home/nw19521/Downloads/gamble.tsv"

l <- read_delim(links.path, delim = "|", col_names = F) %>% 
  separate(X3, c('j', 'links'), sep = "\t") %>% 
  rename(year = X1,
         i = X2) %>% 
  mutate(year = as.double(year),
         ij = paste(i, "_", j))

length(unique(l$ij))

l %>% group_by(year) %>% 
  summarise(n=n())

l %>% filter(year == 2010)

```

In 2010 492,575 rows. 7,868 without NA -- that is both i and j are geolocated --
only 11 distinct ij

In 2002 28,729,384 rows.  28,257,752 without NA -- that is both i and j are geolocated --
only 39 distinct ij


```{r}
l.pc <- l %>% 
  filter(year == 2002) %>% 
  left_join(all, by = c("i" = "host", "year" = "year")) %>% 
  left_join(all, by = c("j" = "host", "year" = "year"))
```

```{r}
l.pc %>% drop_na() %>% distinct(ij)
```

```{r}
length(unique(l.pc$ij))
```

```{r}
sapply(l.pc, function(x) sum(is.na(x)))
```

```{r}
l %>% 
  #filter(year == 2010) %>% 
  mutate(i.host = host_extract(i))

host_extract(l$i)
domain(l$i)
```





Postcode lookup.

[source](https://geoportal.statistics.gov.uk/datasets/postcode-to-output-area-to-lower-layer-super-output-area-to-middle-layer-super-output-area-to-local-authority-district-august-2021-lookup-in-the-uk/about)

```{r}
path.lookup <- paste0(path,"/data/raw/PCD_OA_LSOA_MSOA_LAD_AUG21_UK_LU.csv")
lookup <- read_csv(path.lookup) %>% 
  dplyr::select(pcds, oa11cd) %>% 
  dplyr::rename(pc = pcds)
#glimpse(lookup)
# The problems refer to Welsh LAD names. Not a problem for the analysis.
#sapply(lookup, function(x) sum(is.na(x)))
# 10332 missing msoa11cd
```

The internet archive $1996$-$2010$ data is saved on /hdd.
The internet archive $2011$-$2012$ data is saved on ~/projects/web.diffusion/data/temp.

```{r}
n = 1 #number of unique postcodes. 
m = 12

data.folder <- "/hdd/internet_archive/archive/data/"
data.path9610 <- paste0(data.folder, "domain_pc_year.csv")
#Created by domain.R, which uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df9610 <- read_csv(data.path9610) 

data.path2011_2012 <- paste0(path, "/data/temp/domain_pc_year1112.csv")
#Created by domain1112.Rmd, which is based on domain.R and uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df1112 <- read_csv(data.path2011_2012)

df <- bind_rows(df9610, df1112) %>% 
  filter(
    #V1.domain < m,             # for multiple pc
    V1.domain == n,             # for n == 1
    year > 1995 & year <2013) %>%   
  left_join(lookup, by = "pc", suffix =c("","")) %>% 
  group_by(year, oa11cd) %>%
  #summarise(n = n()) %>%            # for multiple pc  
  summarise(n = sum(V1.domain)) %>%  # for n == 1 
  ungroup()


# tests for Scotland
# df %>% filter(substr(oa11cd, 1,1) =="S",
#               year==2010) %>% summarise(mean(n, na.rm = T))
# 
# df %>% filter(oa11cd=="S00090182")
# 
# test@data %>% filter(substr(id, 1,1) =="S",
#                      n!=0)
# 
# df %>% filter(substr(oa11cd, 1,1) =="W",
#               year==2003)
```