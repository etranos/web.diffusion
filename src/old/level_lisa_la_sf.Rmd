---
title: "diffusion, websites with 1 pc"
date: "`r format(Sys.time(), '%d %B, %Y, %H:%M')`"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_float: true
knit: (function(inputFile, encoding) {
    rmarkdown::render(inputFile, encoding = encoding, output_dir = "../output")
  })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rprojroot)
library(plm)
library(sf)
library(magick)

options(scipen=10000)

# This is the project path
path <- find_rstudio_root_file()
```

## Load data

Postcode lookup.

[source](https://geoportal.statistics.gov.uk/datasets/postcode-to-output-area-to-lower-layer-super-output-area-to-middle-layer-super-output-area-to-local-authority-district-august-2021-lookup-in-the-uk/about)

```{r}
path.lookup <- paste0(path,"/data/raw/PCD_OA_LSOA_MSOA_LAD_AUG21_UK_LU.csv")
lookup <- read_csv(path.lookup) %>% 
  dplyr::select(pcds, oa11cd, lsoa11cd, msoa11cd, ladcd, ladnm) %>% 
  dplyr::rename(pc = pcds)
#glimpse(lookup)
# The problems refer to Welsh LAD names. Not a problem for the analysis.
#sapply(lookup, function(x) sum(is.na(x)))
# 10332 missing msoa11cd
```

The internet archive $1996$-$2010$ data is saved on /hdd.
The internet archive $2011$-$2012$ data is saved on ~/projects/web.diffusion/data/temp.

```{r}
n = 1 #number of unique postcodes. 
m = 12

data.folder <- "/hdd/internet_archive/archive/data/"
data.path9610 <- paste0(data.folder, "domain_pc_year.csv")
#Created by domain.R, which uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df9610 <- read_csv(data.path9610) 

data.path2011_2012 <- paste0(path, "/data/temp/domain_pc_year1112.csv")
#Created by domain1112.Rmd, which is based on domain.R and uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df1112 <- read_csv(data.path2011_2012)

df <- bind_rows(df9610, df1112) %>% 
  filter(
    #V1.domain < m,             # for multiple pc
    V1.domain == n,             # for n == 1
    year > 1995 & year <2013) %>%   
  left_join(lookup, by = "pc", suffix =c("","")) %>% 
  group_by(year, ladcd) %>%
  #summarise(n = n()) %>%            # for multiple pc  
  summarise(n = sum(V1.domain)) %>% # for n == 1 
  ungroup()

df <- df %>% filter(!is.na(ladcd)) %>% 
  complete(ladcd, year) %>% 
  replace(is.na(.), 0)

# tests for Scotland
# df %>% filter(substr(oa11cd, 1,1) =="S",
#               year==2010) %>% summarise(mean(n, na.rm = T))
# 
# df %>% filter(oa11cd=="S00090182")
# 
# test@data %>% filter(substr(id, 1,1) =="S",
#                      n!=0)
# 
# df %>% filter(substr(oa11cd, 1,1) =="W",
#               year==2003)
```

## Firms

```{r, eval=F}
firms <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_142_1.data.csv?geography=1811939329...1811939332,1811939334...1811939336,1811939338...1811939428,1811939436...1811939442,1811939768,1811939769,1811939443...1811939497,1811939499...1811939501,1811939503,1811939505...1811939507,1811939509...1811939517,1811939519,1811939520,1811939524...1811939570,1811939575...1811939599,1811939601...1811939628,1811939630...1811939634,1811939636...1811939647,1811939649,1811939655...1811939664,1811939667...1811939680,1811939682,1811939683,1811939685,1811939687...1811939704,1811939707,1811939708,1811939710,1811939712...1811939717,1811939719,1811939720,1811939722...1811939730,1811939757...1811939767&date=latestMINUS10&industry=37748736&employment_sizeband=0&legal_status=0&measures=20100") %>% 
  dplyr::select(GEOGRAPHY_CODE, OBS_VALUE) %>% #GEOGRAPHY_NAME, 
  rename(ladcd = GEOGRAPHY_CODE,
         firms = OBS_VALUE)

df <- df %>% left_join(firms) %>% 
  mutate(n_by_firm = n/firms) %>% # N websites/firm
  #filter(!is.na(firms)) %>% 
  dplyr::select(-n, -firms) %>% 
  rename(n = n_by_firm)

# df %>% filter(is.na(n_by_firm))
# sapply(df %>% left_join(firms), function(x) sum(is.na(x)))
```

## spatial data

```{r}
# get LA for the UK
path.geo <- paste0(path, "/data/raw/Local_Authority_Districts_(December_2021)_UK_BUC.geojson")
la <- st_read(path.geo)
# source: https://geoportal.statistics.gov.uk/

# spatial transformations
la <- st_transform(la, 4326)

# cities
cities <- maps::world.cities %>% 
  filter(country.etc=="UK") %>% 
  arrange(pop) %>% tail(10) 
cities.sf <- st_as_sf(cities, coords = c("long", "lat"), crs = 4326)
```

## lisa maps

```{r}
path.out <- paste0(path, "/outputs/lisa/la_ggplot/lisa_level_pc", n, "_la") # for unique PC
#path.out <- paste0(path, "/outputs/lisa/la_ggplot/lisa_level_pc", m, "_la")  # for multiple PC

morani <- data.frame()

#spatial weight matrix
sf_use_s2(FALSE)
nb <- poly2nb(la)
listw <- nb2listw(nb, style = "W", zero.policy = T)

for (t in 1996:2012){
  test <- la
  test <- test %>% left_join(df[df$year==t,], by = c("LAD21CD"="ladcd")) %>% 
    mutate(n = ifelse(is.na(n), 0, n),           # 0 for NAs and the same for year
           year = ifelse(is.na(year), t, year))
  
  #spatial weight matrix
  #moved out of the loop

  #Moran's I
  globalMoran <- moran.test(test$n, listw, zero.policy = T, na.action = na.exclude)
  a <- cbind(globalMoran$estimate[[1]],globalMoran$p.value, t[1])
  morani <- rbind(a, morani)

  #LISA
  #lmoran <- cbind(test, localmoran(test$n, listw,  adjust.x=TRUE, zero.policy = T, na.action = na.exclude))
  lmoran <- localmoran(test$n, listw,  adjust.x=TRUE, zero.policy = T, na.action = na.exclude)
  
  quadr <- attr(lmoran, "quadr")
  #quadr[lmoran[,5] > 0.05 | is.na(lmoran[,5]), ] <- NA
  quadr[p.adjust(lmoran[,5], method = "fdr") > 0.05 | is.na(lmoran[,5]), ] <- NA
  
  palette_new <- grDevices::colorRampPalette(
            colors = c("blue", "lightsalmon", "lightblue", "red", "white"))(5)
  
  ggplot(data = la, aes(fill = quadr$median)) +
    geom_sf() +
    scale_fill_manual(values = palette_new, na.value="white", drop = F) +
    theme_minimal() +
    theme(panel.grid.major = element_line(colour = "transparent"),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          legend.title=element_blank()) +
          #legend.position="none") +
    guides(fill = guide_legend(reverse = TRUE)) +
    labs(title = t,
        subtitle = "LISA, websites per firm in a LA")
  
  filename = paste(path.out, t, ".png", sep = "")
  ggsave(filename, bg='#ffffff')
}
```

## Moran I

```{r}
path.out <- paste0(path, "/outputs/lisa/la_ggplot/morani_la.csv")
morani %>% as_tibble() %>% 
  rename('morani'=V1,
         p = V2,
         year = V3) %>% 
  #kable() %>% 
  write_csv(path.out)
```

Both LISA and Moran's I are not the same as before. Double-check the process and 
if okay no need to do the correction as Moran's I is not dropping a lot.
Add legend up left, check grey background, create gif.

https://profrichharris.github.io/MandM/autocorrelation.html#moran-plot-and-local-moran-values