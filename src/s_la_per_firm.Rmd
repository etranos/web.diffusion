---
title: "s.function"
date: "`r format(Sys.time(), '%d %B, %Y, %H:%M')`"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_float: true
knit: (function(inputFile, encoding) {
    rmarkdown::render(inputFile, encoding = encoding, output_dir = "../output")
  })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(broom)
library(rprojroot)
library(rgdal)
library(sf)
library(modelr)
library(survival)
library(rgeos)
library(spdep)
library(rgdal)
library(tidygeocoder)
library(geosphere)
library(raster)
library(rms)
library(stargazer)

options(scipen=10000)

# This is the project path
path <- find_rstudio_root_file()
```

## TODO

- speed, other metrics? 
- understand rsquare
- model LAs
- ~~add SE for the line plots~~ [link](https://stackoverflow.com/questions/26020142/adding-shade-to-r-lineplot-denotes-standard-error). No as we have the points in the plots, so SE is not needed

## Load data

Postcode lookup.

[source](https://geoportal.statistics.gov.uk/datasets/postcode-to-output-area-to-lower-layer-super-output-area-to-middle-layer-super-output-area-to-local-authority-district-august-2021-lookup-in-the-uk/about)

```{r}
path.lookup <- paste0(path,"/data/raw/PCD_OA_LSOA_MSOA_LAD_AUG21_UK_LU.csv")
lookup <- read_csv(path.lookup) %>% 
  dplyr::select(pcds, oa11cd, lsoa11cd, msoa11cd, ladcd, ladnm) %>% 
  dplyr::rename(pc = pcds)
#glimpse(lookup)
# The problems refer to Welsh LAD names. Not a problem for the analysis.
#sapply(lookup, function(x) sum(is.na(x)))
# 10332 missing msoa11cd
```

The internet archive $1996$-$2010$ data is saved on /hdd.
The internet archive $2011$-$2012$ data is saved on ~/projects/web.diffusion/data/temp.

```{r}
n = 1 #number of unique postcodes. 
m = 12

data.folder <- "/hdd/internet_archive/archive/data/"
data.path9610 <- paste0(data.folder, "domain_pc_year.csv")
#Created by domain.R, which uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df9610 <- read_csv(data.path9610) 

data.path2011_2012 <- paste0(path, "/data/temp/domain_pc_year1112.csv")
#Created by domain1112.Rmd, which is based on domain.R and uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df1112 <- read_csv(data.path2011_2012)

df <- bind_rows(df9610, df1112) %>% 
  filter(
    #V1.domain < m,             # for multiple pc
    V1.domain == n,             # for n == 1
    year > 1995 & year <2013) %>%   
  left_join(lookup, by = "pc", suffix =c("","")) %>% 
  group_by(year, ladcd) %>%
  #summarise(n = n()) %>%            # for multiple pc  
  summarise(n = sum(V1.domain)) %>% # for n == 1 
  ungroup()

df <- df %>% filter(!is.na(ladcd)) %>% 
  complete(ladcd, year) %>% 
  replace(is.na(.), 0)

# tests for Scotland
# df %>% filter(substr(oa11cd, 1,1) =="S",
#               year==2010) %>% summarise(mean(n, na.rm = T))
# 
# df %>% filter(oa11cd=="S00090182")
# 
# test@data %>% filter(substr(id, 1,1) =="S",
#                      n!=0)
# 
# df %>% filter(substr(oa11cd, 1,1) =="W",
#               year==2003)
```

## Firms

```{r}
firms <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_142_1.data.csv?geography=1811939329...1811939332,1811939334...1811939336,1811939338...1811939428,1811939436...1811939442,1811939768,1811939769,1811939443...1811939497,1811939499...1811939501,1811939503,1811939505...1811939507,1811939509...1811939517,1811939519,1811939520,1811939524...1811939570,1811939575...1811939599,1811939601...1811939628,1811939630...1811939634,1811939636...1811939647,1811939649,1811939655...1811939664,1811939667...1811939680,1811939682,1811939683,1811939685,1811939687...1811939704,1811939707,1811939708,1811939710,1811939712...1811939717,1811939719,1811939720,1811939722...1811939730,1811939757...1811939767&date=latestMINUS10&industry=37748736&employment_sizeband=0&legal_status=0&measures=20100") %>% 
  select(GEOGRAPHY_CODE, OBS_VALUE) %>% #GEOGRAPHY_NAME, 
  rename(ladcd = GEOGRAPHY_CODE,
         firms = OBS_VALUE)

df <- df %>% left_join(firms) %>% 
  mutate(n_by_firm = n/firms) %>% # N websites/firm
  #filter(!is.na(firms)) %>% 
  select(-n, -firms) %>% 
  rename(n = n_by_firm)

# df %>% filter(is.na(n_by_firm))
# sapply(df %>% left_join(firms), function(x) sum(is.na(x)))
```

## Spatial data

```{r}
# get LA for the UK
path.geo <- paste0(path, "/data/raw/Local_Authority_Districts_(December_2021)_UK_BUC.geojson")
la <- readOGR(path.geo)
# source: https://geoportal.statistics.gov.uk/

# spatial transformations
la <- spTransform(la, CRS("+init=epsg:4326"))

la.f <- fortify(la, region = "LAD21CD")

# cities
cities <- maps::world.cities %>% 
  filter(country.etc=="UK") %>% 
  arrange(pop) %>% tail(10) 
tif_sf <- st_as_sf(cities, coords = c("long", "lat"), crs = 4326)
```

## Help, not really used

```{r, eval=FALSE}

df.test <- df %>% filter(oa11cd == "E00000007" |
                         oa11cd == "E00000021" |
                         oa11cd == "E00000025") %>% 
  mutate(rownum = row_number(),
         value2 = n/max(n))

# function to select unique groups
sample_n_groups = function(grouped_df, size, replace = FALSE, weight=NULL) {
  grp_var <- grouped_df %>% 
    groups %>%
    unlist %>% 
    as.character
  random_grp <- grouped_df %>% 
    summarise() %>% 
    sample_n(size, replace, weight) %>% 
    mutate(unique_id = 1:NROW(.))
  grouped_df %>% 
    right_join(random_grp, by=grp_var) %>% 
    group_by_(grp_var) 
}

df.test <- df %>% group_by(oa11cd) %>% 
  sample_n_groups(10000) %>% 
  arrange(oa11cd, year) %>% 
  select(-unique_id)

fit <- nls(n ~ SSlogis(year, Asym, xmid, scal), data = df.test)
  
fd.nls <- df.test %>% 
  group_by(oa11cd) %>%
  select(year, oa11cd, n) %>% 
  do(fitloop = tidy(nls(n ~ SSlogis(year, Asym, xmid, scal), data = .))) %>% 
  unnest(fitloop)

fd.nls %>% filter(term=="xmid") %>% 
  summarise(rng = range(estimate))
```

## Test for S function estimation

To interpret the coefficients, check [SSlogis](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/SSlogis).
This is a self-starting model, which estimates the starting values. 


```{r}
df.test <- df %>% filter(ladcd == "E06000002") 

fit <- nls(n ~ SSlogis(year, Asym, xmid, scal), data = df.test)
summary(fit)

df.test$prednls = predict(fit)

ggplot(df.test, aes(x = year, y = n) ) +
  geom_point() +
  geom_line(aes(y = prednls), size = 1) +
  scale_x_continuous("year", labels = as.character(df.test$year), breaks = df.test$year) +
  ylab("N. of websites")
  #scale_x_date(breaks = df.test$year) 
```

## S function estimation for all the country

```{r}
df.country <- df %>% group_by(year) %>% 
  summarise(n = mean(n, na.rm = T))

fit <- nls(n ~ SSlogis(year, Asym, xmid, scal), data = df.country)
summary(fit)

xmid.uk <- tidy(fit) %>% filter(term=="xmid") %>% select(estimate) %>% as.numeric()
  
rsquare(fit, df.country)

df.country$prednls = predict(fit)

path.out <- paste0(path, "/outputs/s_uk_per_firm.png")

df.country %>% 
  ggplot(aes(x = year, y = n) ) +
  geom_point() +
  geom_line(aes(y = prednls), size = 1) +
  scale_x_continuous("year", labels = as.character(df.country$year), breaks = df.country$year) +
  geom_vline(xintercept = 2003) +
  ylab("N. of websites / per firm") +
  ggtitle("S-curve, UK") +
  theme(plot.title = element_text(hjust = 0.5)) 

ggsave(path.out)
  #scale_x_date(breaks = df.test$year) 
```

## Loop for S function estimation for Local Authorities

```{r, eval=F}
a.la <- as_tibble()
all.la <- df %>% filter(ladcd != "NA") %>% distinct(ladcd) %>% simplify2array()
simplify2array(all.la)

for (i in all.la){
  data <- df %>% filter(ladcd==i) %>%
    dplyr::select(year, ladcd, n) 
    #lm(n~year, data = .)
    #tryCatch(tidy(nls(n ~ SSlogis(year, Asym, xmid, scal), data = .)), , error=function(e) NULL)
  tryCatch(b.la <- tidy(nls(n ~ SSlogis(year, Asym, xmid, scal), data = data)), error=function(e) NULL)
  tryCatch(a.la <-rbind(a.la,b.la), error=function(e) NULL)
}

a.la %>% filter(term=="xmid") %>% 
  mutate(estimate = round(estimate, 0)) %>% 
  dplyr::select(estimate) %>% 
  summarise(r = range(estimate),
            median = median(estimate))

a.la %>% filter(term=="xmid") %>% 
  mutate(estimate = round(estimate, 0)) %>% 
  filter(p.value < 0.01) %>%    # (227759 - 212782)/227759  # 6% not sig 
  dplyr::select(estimate) %>% 
  ggplot(aes(x=estimate)) + 
  geom_histogram(binwidth = 1) +
  geom_vline(xintercept = 2003, colour = "red") +
  #scale_x_continuous("year", labels = round(a$estimate, 0), breaks = round(a$estimate, 0)) +
  #scale_x_discrete(labels = as.factor(a$estimate))
  theme_minimal() +
  ggtitle("Counts of Xmid per year") +
  theme(plot.title = element_text(hjust = 0.5)) +
  ylab("LAs") + xlab("year")   
  
path.out <- paste0(path, "/outputs/per_firm/xmid_f.png")
ggsave(path.out)  
  
a.la <- a.la %>% filter(term=="xmid") %>% 
  bind_cols(as.tibble(all.la)) %>% 
  mutate(estimate = round(estimate, 0)) %>% 
  filter(p.value < 0.01) %>%    # 376 / 376, all sig
  dplyr::select(ladcd, estimate)
  #filter(estimate>2011)
```

## Loop for S function estimation for Local Authorities keeping the model objects

```{r}
a.la <- tibble()
all.la <- df %>% filter(ladcd != "NA") %>% distinct(ladcd) %>% 
  #top_n(n=2) %>% 
  simplify2array()
simplify2array(all.la)

for (i in all.la){
  data <- df %>% filter(ladcd==i) %>%
    dplyr::select(year, ladcd, n) 
    #lm(n~year, data = .)
    #tryCatch(tidy(nls(n ~ SSlogis(year, Asym, xmid, scal), data = .)), , error=function(e) NULL)
  tryCatch(model <- (nls(n ~ SSlogis(year, Asym, xmid, scal), data = data)), error=function(e) NULL)
  model.name <- paste0("model.", i)
  assign(model.name, model)
  tryCatch(b.la <- tidy(model), error=function(e) NULL)
  
  tryCatch(b.la$r2 <- rsquare(model, data), error=function(e) NULL)
  
  tryCatch(a.la <-rbind(a.la,b.la), error=function(e) NULL)
}

a.la <- a.la %>% filter(term=="xmid") %>% 
  bind_cols(as_tibble(all.la)) %>% 
  #mutate(estimate = round(estimate, 0)) %>% 
  #summarise(r = range(r2))
  filter(p.value < 0.01 & 
         r2 > 0.9) %>%    # 376 / 376, all sig, 330 r2 > 0.9
  dplyr::select(ladcd, estimate, r2, std.error) %>%
  mutate(fast = ifelse(estimate < xmid.uk, "fast", "slow")) # it was 2003


# predictions 
pred <- tibble()
help <- tibble(year = 1996:2012)
for (i in all.la){
  model.name <- (paste0("model.", i))
  #assign(col.name, paste0("pred.", i))
  pred_ <- df %>% filter(ladcd == i) %>% 
    mutate(prednls = eval(call("predict", as.name(model.name))))  # "pred.{i}" :=
  help_ <- help %>% left_join(pred_)  

  pred <- pred %>% bind_rows(help_) 
}

# plots
path.out <- paste0(path, "/outputs/s_lad_per_firm/lad_")

for (i in all.la){
  
  xmid_la <- pred %>% left_join(a.la) %>% filter(ladcd == i) %>% dplyr::select(estimate) %>% first()
  lines <- data.frame(
    intercepts = c(xmid.uk, xmid_la[1]), # it was 2003
    Xmid = c("UK", i)
  )
  
p <- ggplot(pred %>% left_join(a.la) %>% 
              filter(ladcd == i), aes(x = year, y = n, color = fast) ) +
  geom_point() +
  geom_line(aes(y = prednls), size = 1) +
  scale_x_continuous("year", labels = as.character(df$year), breaks = df$year) +
  #geom_vline(xintercept = pred %>% left_join(a.la) %>% filter(ladcd == i) %>% dplyr::select(estimate) %>% first()) +
  ylab("N. of websites / per firm") + 
  labs(color = "Website \ndiffusion speed") +
  labs(title = paste0(la@data %>% filter(LAD21CD== i) %>% dplyr::select(LAD21NM) %>% first(), ", ", i)) +
  theme(legend.position=c(.9,.15)) +
  scale_color_manual(values = c("fast" = "purple",
                                "slow"="orange"),
                     limits = force) +
  geom_vline(
    data=lines,
    aes(xintercept = intercepts, linetype=Xmid),
    size=.5, key_glyph='path', colour="grey30"
    ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) #+
  #geom_vline(xintercept = 2003, colour="grey30", linetype="dashed") 

ggsave(p, filename = paste(path.out, i, ".png", sep = ""), bg="white")
  
}
```

write the object with all the models

```{r eval=F}
path.out <- paste0(path, "/data/temp/s_la_per_firm.csv")
write_csv(a.la, path.out)
```

## Explain fast/slow

### load data

```{r}
a.la <- read_csv(path.out)

library(readxl)
library(httr)
url.classes <- "https://www.ons.gov.uk/file?uri=/methodology/geography/geographicalproducts/areaclassifications/2011areaclassifications/datasets/clustermembershipv2.xls"

GET(url.classes, write_disk(tf <- tempfile(fileext = ".xls", tmpdir = paste0(path, "/data/temp"))))

classes.la <- read_excel(tf, sheet = 3, range = "A10:I399") %>% 
  rename(ladcd = Code,
         supergroup = 'Supergroup Code',
         group = 'Group Code',
         subgroup = 'Subgroup Code')

a.la <- a.la %>% left_join(classes.la) %>% 
  mutate(fast.binary = if_else(fast == "fast", 1, 0))
sapply(a.la, function(x) sum(is.na(x))) # 16 mising

```

### Distances


First set up the destinations. Origins are all the LA.

Major cities from [source](https://www.citypopulation.de/en/uk/cities/).
Retail centres from [source](https://data.cdrc.ac.uk/dataset/retail-centre-boundaries)

```{r}
# LA centroids
la.c <- gCentroid(la, byid=TRUE)@coords %>% 
  as.data.frame() %>%
  bind_cols(la@data) %>% 
  dplyr::select(LAD21CD, x, y) %>% 
  remove_rownames()

# cities
city.names <- c("London, UK", 
"Birmingham, UK",
"Glasgow, UK",
"Liverpool, UK",
"Bristol, UK",
"Manchester, UK",
"Sheffield, UK",
"Leeds, UK",
"Edinburgh, UK",
"Leicester, UK")	

cities <- geo(city.names, no_query = F, method = "arcgis")

# retail centres
geo.path <- paste0(path, "/data/raw/Retail_Boundaries_UK.gpkg")
retail <-readOGR(geo.path) 
#retail.major.cetres <- subset(retail, retail$Classification == "Major Town Centre") #%>% 
retail.major.cetres.help <- gCentroid(retail, byid=TRUE)@coords %>% 
  as.data.frame() %>% 
  add_rownames(var = "id") #%>%

retail.major.cetres <- retail.major.cetres.help %>% 
  st_as_sf(coords = c("x", "y"), crs = 27700) %>%
  st_transform(4326) %>%
  st_coordinates() %>%
  as_tibble() %>% 
  bind_cols(retail.major.cetres.help$id) %>% 
  rename(id = '...3') %>% 
  left_join(retail@data %>% mutate(id = rownames(retail@data)), by = 'id')
```

Distance to cities

```{r}
dist <- distm(cbind(la.c$x, la.c$y), cbind(cities$long, cities$lat), fun=distHaversine) 
dist <- round((dist/1000),2) %>% 
  as_tibble()  

city.names <- city.names %>% stringr::str_remove(", UK")
names(dist) <- city.names 

dist <- dist %>% bind_cols(la.c$LAD21CD) %>% 
  rename(ladcd = last_col()) %>% 
  relocate(ladcd)

# dist$dist <- names(dist)[apply(dist[-1], MARGIN = 1, FUN = which.min)]
# dist$distMet <- apply(dist[,2:11], 1, min)

dist <- transform(dist, dist = do.call(pmin, dist[-1]))



# NOT USED AS IT SCREWS UP THE REGRESSIONS
# Complete panel, N = 3871903, 17 year * 227759 OA
# df <- df %>% bind_rows(dist %>% select(oa11cd)) %>% 
#   mutate(year = ifelse(is.na(year),1000, year),
#          country = substring(oa11cd, 1, 1)) %>% 
#   filter(country != "L" & country != "M" & country != "N") %>% 
#   filter(!is.na(country)) %>% 
#   complete(year, oa11cd, fill = list(n = 0)) %>% 
#   filter(year!=1000,
#          !is.na(oa11cd)) %>% 
#   select(-country)



# tests
# test$country <- substring(test$oa11cd, 1, 1)
# unique(test$country)
# length(unique(test$oa11cd))

# calculate area
la$SHAPE_Area <- area(la) #/ 1000000

# Join with distance and area
a.la <- a.la %>% left_join(dist, by = c("ladcd" = "ladcd")) 
sapply(a.la, function(x) sum(is.na(x)))
```

Distance to retail centres

```{r}
dist.retail <- distm(cbind(la.c$x, la.c$y), cbind(retail.major.cetres$X, retail.major.cetres$Y), fun=distHaversine) 
dist.retail <- round((dist.retail/1000),2) %>% 
  as_tibble()  

# retail.names <- retail.major.cetres$RC_Name
# names(dist.retail) <- retail.names 

dist.retail <- dist.retail %>% bind_cols(la.c$LAD21CD) %>% 
  rename(LAD21CD = last_col()) %>% 
  relocate(LAD21CD)

# Minimum distance 
dist.retail <- transform(dist.retail, dist.retail = do.call(pmin, dist.retail[-1]))
dist.retail <- dist.retail %>% dplyr::select(LAD21CD, dist.retail)

# Join with complete panel
a.la <- a.la %>% left_join(dist.retail, by = c("ladcd" = "LAD21CD")) 

sapply(a.la, function(x) sum(is.na(x)))
```


### Regressions

**TODO** add distance, see what classes mean, try lm

```{r}

model <- glm(fast.binary ~ log(London + 0.001) + log(dist + 0.001) + log(dist.retail + 0.001) + as.factor(subgroup), family=binomial (link='logit'), data = a.la)
summary(model)
stargazer(model, type = 'text')
```

For Nagelkerke R2

```{r}
model2 <- lrm(fast.binary ~ log(London + 0.001) + log(dist + 0.001) + log(dist.retail + 0.001) + subgroup, data = a.la)
print(model2)
```


## Survival

- pop.density
- income
- firms
- employment
- N/S
- London

Time series formula will look like this:

`coxph(Surv(tstart, tstop, infect) ~ treat + inherit + steroids, data =newcgd, cluster = id)`

This is what @perkins2005international should be using.
See p. 7 [here](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf).

using `library(survival)`

```{r}
test.surv.data <- a.la %>% mutate(status = 1) %>% 
  left_join(data.la, by = c("ladcd" = "LAD21CD")) %>% 
  left_join(df %>% filter(year == 2012), by = "ladcd")

cf <- coxph(Surv((estimate-1996), status) ~ log(distLondon) + log(distMet) + log(pop16_64) + log(pop.dens) + log(econ.act) + log(earnings) + log(total.busi) + south + log(n), data=test.surv.data, weights = 1/(std.error)^2)
summary(cf)

```

## Map the xmid

```{r}

la.f <- la.f %>% left_join(a.la, by = c("id" = "ladcd")) %>% 
  mutate(fast = ifelse(estimate < 2003, "fast", "slow")) %>% 
  arrange(order)  # if i don't order merge.nuts.f loses order and the map has gaps


# oa.gb.f %>% mutate(fast = ifelse(estimate < 2003, 1, 0)) %>% 
#   arrange(order) %>% 
ggplot(data = la.f, aes(x = long, y = lat)) +
  geom_polygon(aes(group = group, fill = estimate)) + #fill = fast, 
  #theme_nothing(legend = TRUE) +
  labs(title = "Clusters") +
  scale_fill_continuous(type = "viridis") +
  theme_void()

path.out <- paste0(path, "/outputs/speed_pam.png")
ggplot(data = la.f, aes(x = long, y = lat)) +
  geom_polygon(aes(group = group, fill = fast)) + #fill = fast, 
  scale_fill_manual(values = c("darkorchid3", "orange"), #darkmagenta, purple4
                    na.value = "grey70") + 

  #theme_nothing(legend = TRUE) +
  labs(title = "Website diffusion speed in Local authorities") +
  geom_point(data = cities, aes(x=long, y=lat), colour = "black", size = .1) + 
  geom_text(size = 2, check_overlap = F, nudge_y = .2, data = cities, aes(x=long, y=lat, label = name)) +
  #scale_fill_continuous(type = "viridis") +
  labs(fill='Diffusion') +
  theme_void() +
  theme(plot.title = element_text(hjust = 0.5)) 
ggsave(path.out)
```

```{r}
la.f %>% left_join(df %>% filter(year==2012), by = c("id" = "ladcd")) %>% 
  arrange(order) %>% # if i don't order merge.nuts.f loses order and the map has gaps
  ggplot(aes(x = long, y = lat)) +
  geom_polygon(aes(group = group, fill = n)) + #fill = fast, 
  #theme_nothing(legend = TRUE) +
  labs(title = "Clusters") +
  scale_fill_continuous(type = "viridis") +
  theme_void()

```

## time series clusters

2 clusters is the preferred solution, but when I plot individual local authorities 
I cannot interpret the differences between the two clusters

```{r}
library(dtwclust)
library(tsbox)
library(imputeTS)


# turn to a wide format
ts.wide <- df %>% 
  filter(!is.na(ladcd)) %>% 
  rename(time = year) %>%
  dplyr::select(time, ladcd, n) %>% 
  #mutate(year = )
  ts_wide()

# impute with means of each TS
# TODO check sensitivity
ts.wide <- na_mean(ts.wide)
#sapply(ts.wide, function(x) sum(is.na(x)))

# extract names
ts.wide.names <- names(ts.wide[,-1])
ts.wide <- ts.wide[,-1]

# standardise
ts.wide <- zscore(ts.wide)

# convert to list for tsclust
ts.wide <- split(ts.wide, rep(1:ncol(ts.wide), each = nrow(ts.wide)))

# 5 - 20 cluster sollutions
pc_k <- tsclust(ts.wide, k = c(2:10), seed = 94L,
                type="partitional",
                distance = "dtw_basic", centroid = "pam", trace = T, 
                args = tsclust_args(dist = list(window.size = 20L)))

# decide k, see RJ-2019-023 for max/min
#names(pc_k) <- paste0("k_", c("5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15"))
names(pc_k) <- paste0("k_", c("2", "3", "4", "5", "6", "7", "8", "9", "10"))
cvis <- sapply(pc_k, cvi, type = "internal")
cvis
# max Sil
# max SF
# max CH
# min DB
# min DBstar
# max D
# min COP

# loop for k decision
pref.cvi <- NULL

max.cvis <- c(1:3,6)
for (i in max.cvis){
  a <- which.max(cvis[i,])
  pref.cvi <- c(pref.cvi, a)
}

min.cvis <- c(4:5,7)
for (i in min.cvis){
  a <- which.min(cvis[i,])
  pref.cvi <- c(pref.cvi, a)
}

print(pref.cvi) # just count how many time each k_* appears, this is the preferred solution 

pc_k <- tsclust(ts.wide, k = 7, seed = 94L,  # 7 or 2
                type="partitional", 
                distance = "dtw_basic", centroid = "pam", trace = T, 
                args = tsclust_args(dist = list(window.size = 20L)))
plot(pc_k)

clusters <- data.frame(LAD=ts.wide.names,
                           cluster=pc_k@cluster)

```

## Mapping the clusters

```{r include=TRUE, echo=FALSE, results= 'markup', message=FALSE, warning = FALSE, fig.height=10, fig.width=10}

la.f <- la.f %>% left_join(clusters, by = c("id" = "LAD")) %>% 
  arrange(order)  # if i don't order merge.nuts.f loses order and the map has gaps

ggplot(la.f %>% filter(!is.na(r2)), aes(x = long, y = lat)) +
  geom_polygon(aes( group = group, fill = as.factor(cluster))) +
  #theme_nothing(legend = TRUE) +
  labs(title = "Clusters") +
  scale_fill_viridis_d()+
  theme_void()

# ggplot(la.f, aes(x = long, y = lat)) +
#   geom_polygon(aes( group = group, fill = as.factor(cluster.down))) +
#   theme_nothing(legend = TRUE) +
#   labs(title = "Download clusters") +
#   scale_fill_viridis_d()+
#   theme_void()
# 
# cluster.upL <- c(6, 9, 11, 12, 13)
# cluster.upS <- c(1, 2, 3, 7, 8, 10)
#  
# la.f_ <- la.f #plot the background
# 
# cluster.upL.plot <- la.f %>% 
#   filter(cluster.up %in% cluster.upL) %>% 
#   ggplot(aes(x = long, y = lat)) +
#   geom_polygon(data = la.f_, color='grey', fill='white', aes(x=long, y=lat, group=group, colour = 'grey30')) +
#   geom_polygon(aes( group = group, fill = as.factor(cluster.up))) +
#   labs(title = "Larger clusters, upload speeds") + 
#   scale_fill_viridis_d() +
#   guides(fill=guide_legend(title="Clusters")) +
#   theme_void()
# 
# cluster.upS.plot <- la.f %>% 
#   filter(cluster.up %in% cluster.upS) %>% 
#   ggplot(aes(x = long, y = lat)) +
#   geom_polygon(data = la.f_, color='grey', fill='white', aes(x=long, y=lat, group=group, colour = 'grey30')) +
#   geom_polygon(aes( group = group, fill = as.factor(cluster.up))) +
#   labs(title = "Smaller clusters, upload speeds") + 
#   scale_fill_viridis_d() +
#   guides(fill=guide_legend(title="Clusters")) +
#   theme_void()
# 
# plot_grid(cluster.upL.plot, cluster.upS.plot, nrow = 1,
#           align = "h", axis = "l", 
#           #labels = c(".co.uk / pop., 2000", ".co.uk / pop., 2010", "non .co.uk / pop., 2000", "non .co.uk / pop., 2010", ""), 
#           #label_size = 12,
#           greedy = T)
#           #rel_heights = c(1, 1, .3)) # this is the relative position of the 3 lines
# 
# #ggsave("all_maps2.png") # this is the colour map for the preprint
# out.path.map <- paste0(path, "/paper/v1/figures/map.up.clusters.png")
# ggsave(out.path.map)#,
#        #width = 210, #A4
#        #height = 297, #A4
#        #units = "mm")
```