---
title: "s.function for 10 postcodes"
date: "`r format(Sys.time(), '%d %B, %Y, %H:%M')`"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_float: true
knit: (function(inputFile, encoding) {
    rmarkdown::render(inputFile, encoding = encoding, output_dir = "../output")
  })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(broom)
library(rprojroot)
library(rgdal)
library(sf)
library(modelr)
library(ggbump)
library(ggrepel)

options(scipen=10000)

# This is the project path
path <- find_rstudio_root_file()
```

## Load data

Postcode lookup.

[source](https://geoportal.statistics.gov.uk/datasets/postcode-to-output-area-to-lower-layer-super-output-area-to-middle-layer-super-output-area-to-local-authority-district-august-2021-lookup-in-the-uk/about)

```{r}
path.lookup <- paste0(path,"/data/raw/PCD_OA_LSOA_MSOA_LAD_AUG21_UK_LU.csv")
lookup <- read_csv(path.lookup) %>% 
  dplyr::select(pcds, oa11cd, lsoa11cd, msoa11cd, ladcd, ladnm) %>% 
  dplyr::rename(pc = pcds)
#glimpse(lookup)
# The problems refer to Welsh LAD names. Not a problem for the analysis.
#sapply(lookup, function(x) sum(is.na(x)))
# 10332 missing msoa11cd
```

The internet archive $1996$-$2010$ data is saved on /hdd.
The internet archive $2011$-$2012$ data is saved on ~/projects/web.diffusion/data/temp.

## Problem postcodes

I use the problem postcodes from n = 1 as these are the true outliers as per the
line plots.

```{r}
n = 1 #number of unique postcodes. 
m = 11

data.folder <- "/hdd/internet_archive/archive/data/"
data.path9610 <- paste0(data.folder, "domain_pc_year.csv")
#Created by domain.R, which uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df9610 <- read_csv(data.path9610) %>% 
  filter(
      V1.domain < m,               # for multiple pc
      #V1.domain == n,             # for n == 1
      year > 1995)   

data.path2011_2012 <- paste0(path, "/data/temp/domain_pc_year1112.csv")
#Created by domain1112.Rmd, which is based on domain.R and uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df1112 <- read_csv(data.path2011_2012) %>% 
  filter(
    V1.domain < m)               # for multiple pc
    #V1.domain == n)             # for n == 1

df.long <- bind_rows(df9610, df1112) %>% 
  filter(year > 1995 & year < 2013) %>% 
  group_by(pc, year) %>% 
  summarise(n = sum(V1.domain)) %>%
  ungroup() %>% 
  #arrange(desc(n)) %>% 
  pivot_wider(names_from = year, values_from = n, names_prefix = "n_") %>% 
  dplyr::select(pc, n_1996, n_1999, n_2000, n_2001, n_2002, n_2003, n_2004, n_2005, n_2006, n_2007, n_2008, n_2009, n_2010, n_2011, n_2012) %>%
  arrange(desc(n_2004), desc(n_2005)) %>% 
  slice_head(n=1000) %>% 
  pivot_longer(!pc, names_to = "year", values_to = "n") %>% 
  mutate(year = as.integer(sub("n_", "", year, fixed = TRUE))) %>% 
  replace(is.na(.), 0) %>% 
  mutate(outlier = ifelse(n > 1000 & (year==2004 | year == 2005), pc, ""))

# From n = 1 as these are the 'true' outliers
problem.pcs <- c("SE24 9HP", "CV8 2ED", "GL16 7YA", "CW1 6GL", "M28 2SL", "DE21 7BF")
```

## Corrected

The above exploratory analysis indicated that there is a huge increase in 2004-2006
in the above postcodes. I turn them to NAs and then use a regression to impute these
gaps.

```{r}
df.corrected <- bind_rows(df9610, df1112) %>% 
  filter(year > 1995 & year < 2013) %>% 
  group_by(pc, year) %>% 
  summarise(n = sum(V1.domain)) %>% 
  ungroup() %>% 
  # The next line is different than the same for  n = 1: problem.pcs$pc
  mutate(n = ifelse(pc %in% as.character(problem.pcs) & year > 2001 & year <2007, NA, n))

df.corrected <- pdata.frame(df.corrected)
predictions <- round(predict(plm(n ~ as.factor(year), effect = "individual", 
                                 model = "within", data = df.corrected), newdata = df.corrected), 2)

df.corrected$n[is.na(df.corrected$n)] <- predictions[is.na(df.corrected$n)]

```

## LA df

```{r}
df <- df.corrected %>% 
  mutate(year = as.integer(as.character(year))) %>% 
  filter(year > 1995 & year < 2013) %>% 
  left_join(lookup, by = "pc", suffix =c("","")) %>% 
  group_by(year, ladcd) %>%
  summarise(n = sum(n)) %>% # for multiple pc  **********ATTENTION*********
  ungroup() %>% 
  complete(ladcd, year) %>% 
  filter(!is.na(ladcd)) %>% # drop c. 10 - 200 per year, which are not assigned to a LAD
  replace(is.na(.), 0) 
```

## Firms

```{r, eval=T}

# also saved locally in /data/raw/firm_data_backup.csv

firms <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_142_1.data.csv?geography=1811939329...1811939332,1811939334...1811939336,1811939338...1811939428,1811939436...1811939442,1811939768,1811939769,1811939443...1811939497,1811939499...1811939501,1811939503,1811939505...1811939507,1811939509...1811939517,1811939519,1811939520,1811939524...1811939570,1811939575...1811939599,1811939601...1811939628,1811939630...1811939634,1811939636...1811939647,1811939649,1811939655...1811939664,1811939667...1811939680,1811939682,1811939683,1811939685,1811939687...1811939704,1811939707,1811939708,1811939710,1811939712...1811939717,1811939719,1811939720,1811939722...1811939730,1811939757...1811939767&date=latestMINUS10&industry=37748736&employment_sizeband=0&legal_status=0&measures=20100") %>% 
  dplyr::select(GEOGRAPHY_CODE, OBS_VALUE) %>% #GEOGRAPHY_NAME, 
  rename(ladcd = GEOGRAPHY_CODE,
         firms = OBS_VALUE)

df <- df %>% left_join(firms) %>% 
  mutate(n_by_firm = n/firms) %>% # N websites/firm
  #filter(!is.na(firms)) %>% 
  dplyr::select(-n, -firms) %>% 
  rename(n = n_by_firm)

# df %>% filter(is.na(n_by_firm))
# sapply(df %>% left_join(firms), function(x) sum(is.na(x)))
```

## Spatial data

```{r}
# get LA for the UK
path.geo <- paste0(path, "/data/raw/Local_Authority_Districts_(December_2021)_UK_BUC.geojson")
la <- st_read(path.geo)
# source: https://geoportal.statistics.gov.uk/

# spatial transformations
la <- st_transform(la, 4326)

#la.f <- fortify(la, region = "LAD21CD")

# cities
cities.1 <- maps::world.cities %>% 
  filter(country.etc=="UK") %>% 
  slice_max(pop, n = 10)
cities.2 <- maps::world.cities %>% 
  filter(country.etc=="UK") %>% 
  filter(name == "Belfast")

cities.sf <- bind_rows(cities.1, cities.2) %>% 
  st_as_sf(coords = c("long", "lat"), crs = 4326)
```

## S function estimation for all the country

To interpret the coefficients, check [SSlogis](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/SSlogis).
This is a self-starting model, which estimates the starting values. 

```{r}
df.country <- df.corrected %>% 
  mutate(year = as.integer(as.character(year))) %>% 
  filter(year > 1995 & year < 2013) %>% 
  left_join(lookup, by = "pc", suffix =c("","")) %>% 
  group_by(year, ladcd) %>%
  summarise(n = sum(n)) %>% # for multiple pc  **********ATTENTION*********
  ungroup() %>% 
  complete(ladcd, year) %>% 
  filter(!is.na(ladcd)) %>% # drop c. 10 - 200 per year, which are not assigned to a LAD
  replace(is.na(.), 0) %>% 
  left_join(firms) %>% 
  group_by(year) %>% 
  summarise(n = sum(n, na.rm = T),
            firms = sum(firms, na.rm = T)) %>% 
  mutate(n = n/firms) %>% 
  dplyr::select(-firms)

fit <- nls(n ~ SSlogis(year, Asym, xmid, scal), data = df.country)
summary(fit)

# xmid = 2002.303
xmid.uk <- tidy(fit) %>% 
  filter(term=="xmid") %>% 
  dplyr::select(estimate) %>% as.numeric()
  
rsquare(fit, df.country)

df.country$prednls = predict(fit)

path.out <- paste0(path, "/outputs/s/s_uk_per_firm_10.png")

df.country %>% 
  ggplot(aes(x = year, y = n) ) +
  geom_point() +
  geom_line(aes(y = prednls), size = 1) +
  scale_x_continuous(NULL, labels = as.character(df.country$year), breaks = df.country$year) +
  geom_vline(xintercept = xmid.uk) +
  ylab("N. of websites per firm") +
  #ggtitle("S-curve, UK") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
          panel.grid.minor.x = element_blank()) 

ggsave(path.out)
  #scale_x_date(breaks = df.test$year) 
```


## Loop for S function estimation for Local Authorities keeping the model objects

```{r}
a.la <- tibble()
all.la <- df %>% filter(ladcd != "NA") %>% distinct(ladcd) %>% 
  #top_n(n=2) %>% 
  simplify2array()
simplify2array(all.la)

for (i in all.la){
  data <- df %>% filter(ladcd==i) %>%
    dplyr::select(year, ladcd, n) 
    #lm(n~year, data = .)
    #tryCatch(tidy(nls(n ~ SSlogis(year, Asym, xmid, scal), data = .)), , error=function(e) NULL)
  tryCatch(model <- (nls(n ~ SSlogis(year, Asym, xmid, scal), 
                         data = data, 
                         control = nls.control(maxiter = 1000))), error=function(e) NULL)
  model.name <- paste0("model.", i)
  assign(model.name, model)
  tryCatch(b.la <- tidy(model), error=function(e) NULL)
  
  tryCatch(b.la$r2 <- rsquare(model, data), error=function(e) NULL)
  
  tryCatch(a.la <-rbind(a.la,b.la), error=function(e) NULL)
}

a.la <- a.la %>% filter(term=="xmid") %>% 
  bind_cols(as_tibble(all.la)) %>% 
  #mutate(estimate = round(estimate, 0)) %>% 
  #summarise(r = range(r2))
  filter(p.value < 0.01 & 
         r2 > 0.9) %>%    # 376 / 376, all sig, 330 r2 > 0.9
  dplyr::select(ladcd, estimate, r2, std.error) %>%
  mutate(fast = ifelse(estimate < xmid.uk, "fast", "slow")) # it was 2003


# predictions 
pred <- tibble()
help <- tibble(year = 1996:2012)
for (i in all.la){
  model.name <- (paste0("model.", i))
  #assign(col.name, paste0("pred.", i))
  pred_ <- df %>% filter(ladcd == i) %>% 
    mutate(prednls = eval(call("predict", as.name(model.name))))  # "pred.{i}" :=
  help_ <- help %>% left_join(pred_)  

  pred <- pred %>% bind_rows(help_) 
}

# plots
path.out <- paste0(path, "/outputs/s/s_lad_per_firm_10/lad_10_")

for (i in all.la){
  
  xmid_la <- pred %>% left_join(a.la) %>% filter(ladcd == i) %>% dplyr::select(estimate) %>% first() #******
  lines <- data.frame(
    intercepts = c(xmid.uk, xmid_la[[1]]), # ******************
    Xmid = c("UK", i)
  )
  
p <- ggplot(pred %>% left_join(a.la) %>% 
              filter(ladcd == i), aes(x = year, y = n, color = fast) ) +
  geom_point() +
  geom_line(aes(y = prednls), size = 1) +
  scale_x_continuous(NULL, labels = as.character(df$year), breaks = df$year) +
  #geom_vline(xintercept = pred %>% left_join(a.la) %>% filter(ladcd == i) %>% dplyr::select(estimate) %>% first()) +
  ylab("N. of websites / per firm") + 
  labs(color = "Website \ndiffusion speed") +
  labs(title = paste0(la  %>% as_tibble() %>% filter(LAD21CD== i) %>% dplyr::select(LAD21NM) %>% first(), ", ", i)) +
  theme(legend.position=c(.9,.15)) +
  scale_color_manual(values = c("fast" = "purple",
                                "slow"="orange"),
                     limits = force) +
  geom_vline(
    data=lines,
    aes(xintercept = intercepts, linetype=Xmid),
    size=.5, key_glyph='path', colour="grey30"
    ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) #+
  #geom_vline(xintercept = 2003, colour="grey30", linetype="dashed") 

ggsave(p, filename = paste(path.out, i, ".png", sep = ""), bg="white")
}
```

Write the object with all the models. 
NAs are for LAD with R^2 < 0.9.

```{r eval=F}
path.out <- paste0(path, "/outputs/s/s_la_per_firm_10.csv")

la %>% as_tibble() %>% 
  dplyr::select(LAD21CD, LAD21NM) %>% 
  left_join(a.la, by = c("LAD21CD" = "ladcd")) %>% 
  write_csv(path.out)
```

## Map the xmid

```{r}
path.out <- paste0(path, "/outputs/s/speed_map_10.png")

la %>% left_join(a.la, by = c("LAD21CD" = "ladcd")) %>% 
  ggplot(aes(fill = fast)) +
  geom_sf() +  
  #theme_nothing(legend = TRUE) +
  theme_void() +
  labs(fill='Diffusion speed') +
  scale_fill_manual(values = c("darkorchid3", "orange"), #darkmagenta, purple4
                    na.value = "grey70") 

ggsave(path.out)
```

## Average ranks websites per firm t and 2012 only

Change percentile in out.path and mutate(alpha = ifelse(abs(diff_1st)

```{r}

t <- 2000

out.path <- paste0(path, "/outputs/ranks/web_per_firm", t,"_2012_only05_av_10.png")

df %>% 
  
  left_join(la %>% as_tibble() %>% dplyr::select(LAD21CD, LAD21NM),
            by = c("ladcd" = "LAD21CD")) %>% 
  
  mutate(period = ifelse(year < 1999, 1, 
                         ifelse(year > 2009, 3, 2))) %>%
  group_by(ladcd, period) %>%
  mutate(n = ifelse(period != 2, mean(n), n)) %>% 
  filter(year > 1997 & year < 2011) %>% 
  group_by(year) %>% 
  mutate(rank = rank(desc(n))) %>% 
  ungroup() %>% 
  filter(year==1998 | year == 2010) %>% 
  arrange(ladcd, year) %>% 
  group_by(ladcd) %>% 
  #mutate(diff_1st = -diff(rank))
  mutate(diff_1st = -(rank-dplyr::lag(rank))) %>% 
  
  # ungroup() %>% summarise(q = quantile(abs(diff_1st), .95, na.rm = T))
  # q.95 = 194.5, q.9 = 162.5, q.1 = 10.5, q.05 = 5. **abs()**

  #mutate(alpha = ifelse(abs(diff_1st) > 194.5, .8, 0.1)) %>% 
  mutate(alpha = ifelse(abs(diff_1st) <= 5, .8, 0.1)) %>% 
  
  mutate(alpha = ifelse(is.na(diff_1st), 0.1, alpha)) %>%
  mutate(label = ifelse(alpha==.1, "", LAD21NM)) %>% 

  ggplot(aes(year, rank, group = ladcd, alpha=alpha)) +
  geom_bump(colour = "red") +
  scale_y_reverse(labels = c(1, 100, 200, 300, 400)) +
  # geom_text(#data = df %>% filter(year == 2012),
  #           aes(x = Inf, label = label), size = 4, hjust = 0)  # x = 2012 + .1
  geom_text_repel(aes(label = label),
                  na.rm = TRUE, 
                  size = 2,
                  nudge_x = 2, nudge_y = 1,
                  direction = "y", hjust = "left") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position="none") +
  scale_x_continuous(NULL, 
                     #limits = c(1998, 2010), 
                     breaks=c(1998, 2010),
                     labels = c("1996-98","2010-12"))
ggsave(out.path)
```

## Average ranks websites per firm t and 2012 only: 05 and 95

```{r}

t <- 2000

out.path <- paste0(path, "/outputs/ranks/web_per_firm", t,"_2012_only0595_av_10.png")

df %>% 
  
  left_join(la %>% as_tibble() %>% dplyr::select(LAD21CD, LAD21NM),
            by = c("ladcd" = "LAD21CD")) %>% 
  
  mutate(period = ifelse(year < 1999, 1, 
                         ifelse(year > 2009, 3, 2))) %>%
  group_by(ladcd, period) %>%
  mutate(n = ifelse(period != 2, mean(n), n)) %>% 
  filter(year > 1997 & year < 2011) %>% 
  group_by(year) %>% 
  mutate(rank = rank(desc(n))) %>% 
  ungroup() %>% 
  filter(year==1998 | year == 2010) %>% 
  arrange(ladcd, year) %>% 
  group_by(ladcd) %>% 
  #mutate(diff_1st = -diff(rank))
  mutate(diff_1st = -(rank-dplyr::lag(rank))) %>% 
  
  # ungroup() %>% summarise(q = quantile(abs(diff_1st), .05, na.rm = T))
  # q.95 = 194.5, q.9 = 162.5, q.1 = 10.5, q.05 = 5. **abs()**

  mutate(alpha = ifelse(abs(diff_1st) <= 5 | abs(diff_1st) > 194.5, 1, 0.5)) %>% 
  mutate(alpha = ifelse(is.na(diff_1st), 0.5, alpha)) %>%
  
  mutate(q = ifelse(abs(diff_1st) <= 5, "05",
                    ifelse(abs(diff_1st) > 194.5, "95", "m"))) %>% 
  mutate(q = ifelse(is.na(diff_1st), "m", q)) %>%
  
  ggplot(aes(year, rank, group = ladcd, alpha=alpha)) +
  geom_bump(aes(colour = q)) + #colour = "red"
  scale_color_manual(values = c("darkorchid3", "orange", "grey70"),
                     breaks = c("05", "95"),
                     labels = c("Stability","Volatility")) + 
  labs(color = "LAD") +
  scale_y_reverse(labels = c(1, 100, 200, 300, 400)) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  #theme(legend.position="none") +
  scale_x_continuous(NULL, 
                     #limits = c(1998, 2010), 
                     breaks=c(1998, 2010),
                     labels = c("1996-98","2010-12")) +
  guides(alpha = "none")

ggsave(out.path)
```


