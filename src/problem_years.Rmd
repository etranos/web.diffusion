---
title: "problem years"
author: "Emmanouil Tranos"
date: '2024-05-17'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rprojroot)
library(plm)
library(sf)
library(magick)

options(scipen=10000)

# This is the project path
path <- find_rstudio_root_file()
```

## Load data

Postcode lookup.

[source](https://geoportal.statistics.gov.uk/datasets/postcode-to-output-area-to-lower-layer-super-output-area-to-middle-layer-super-output-area-to-local-authority-district-august-2021-lookup-in-the-uk/about)

```{r}
path.lookup <- paste0(path,"/data/raw/PCD_OA_LSOA_MSOA_LAD_AUG21_UK_LU.csv")
lookup <- read_csv(path.lookup) %>% 
  dplyr::select(pcds, oa11cd, lsoa11cd, msoa11cd, ladcd, ladnm) %>% 
  dplyr::rename(pc = pcds)
#glimpse(lookup)
# The problems refer to Welsh LAD names. Not a problem for the analysis.
#sapply(lookup, function(x) sum(is.na(x)))
# 10332 missing msoa11cd
```

The internet archive $1996$-$2010$ data is saved on /hdd.
The internet archive $2011$-$2012$ data is saved on ~/projects/web.diffusion/data/temp.

```{r}

data.path2011_2012 <- paste0(path, "/data/temp/domain_pc_year1112.csv")
#Created by domain1112.Rmd, which is based on domain.R and uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df1112 <- read_csv(data.path2011_2012)

df <- bind_rows(df9610, df1112) %>% 
  filter(
    #V1.domain < m,             # for multiple pc
    V1.domain == n,             # for n == 1
    year > 1995 & year <2013) %>%   
  left_join(lookup, by = "pc", suffix =c("","")) %>% 
  group_by(year, ladcd) %>%
  #summarise(n = n()) %>%            # for multiple pc  
  summarise(n = sum(V1.domain)) %>% # for n == 1 
  ungroup()

df <- df %>% filter(!is.na(ladcd)) %>% 
  complete(ladcd, year) %>% 
  replace(is.na(.), 0)

# tests for Scotland
# df %>% filter(substr(oa11cd, 1,1) =="S",
#               year==2010) %>% summarise(mean(n, na.rm = T))
# 
# df %>% filter(oa11cd=="S00090182")
# 
# test@data %>% filter(substr(id, 1,1) =="S",
#                      n!=0)
# 
# df %>% filter(substr(oa11cd, 1,1) =="W",
#               year==2003)
```

### Problem postcodes

```{r}
n = 1 #number of unique postcodes. 
m = 12

data.folder <- "/hdd/internet_archive/archive/data/"
data.path9610 <- paste0(data.folder, "domain_pc_year.csv")
#Created by domain.R, which uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df9610 <- read_csv(data.path9610) %>% 
  filter(
      #V1.domain < m,             # for multiple pc
      V1.domain == n,             # for n == 1
      year > 1995)   

# df9610 %>% filter(V1.domain == 1) %>% 
#   left_join(lookup, by = "pc", suffix =c("","")) %>% 
#   filter(ladcd == "E08000006" & year == 2006) %>% 
#   group_by(pc) %>% 
#   summarise(n = n()) %>% 
#   arrange(desc(n))
# 
# df9610 %>% filter(pc == "M28 2SL" & year == 2006 & V1.domain == 1)

problem.pcs <- df9610 %>% filter(year > 1999 & year < 2011) %>% 
  group_by(pc, year) %>% 
  summarise(n = n()) %>%
  ungroup() %>% 
  # complete(pc, year) %>% 
  # replace(is.na(n), 0) %>% 
  arrange(desc(n)) %>% 
  pivot_wider(names_from = year, values_from = n, names_prefix = "n_") %>% 
  dplyr::select(pc, n_2000, n_2001, n_2002, n_2003, n_2004, n_2005, n_2006, n_2007, n_2008, n_2009, n_2010) %>% 
  slice_head(n=10) %>% 
  dplyr::select(pc)

# [1] "M28 2SL"  "SE24 9HP" "CV8 2ED"  "CW1 6GL"  "EC1V 4PY" "DE21 7BF" "E14 9SR"  "GL16 7YA" "WC1N 3XX"
# [10] "CB2 1AB" 
```

### Correct 1996-2010

The above exploratory analysis indicated that there is a huge increase in 2004-2006
in the above postcodes. I turn them to NAs and then use a regression to impute these
gaps.

```{r}
df9610 <- df9610 %>% 
  group_by(pc, year) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  mutate(n = ifelse(pc %in% as.character(problem.pcs$pc) & year > 2003 & year <2007, NA, n))

df9610 <- pdata.frame(df9610)
predictions <- round(predict(plm(n ~ as.factor(year), effect = "individual", 
                                 model = "within", data = df9610), newdata = df9610), 2)

df9610$n[is.na(df9610$n)] <- predictions[is.na(df9610$n)]
df9610 %>% filter(pc == "M28 2SL") %>% distinct(year)

df9610 <- as_tibble(df9610) %>% 
  mutate(year = as.double(as.character(year))) %>% 
  left_join(lookup, by = "pc", suffix =c("","")) %>%
  group_by(year, ladcd) %>%
  summarise(n = n()) %>%            
  ungroup()
```

### Load remaining data

```{r}
data.path2011_2012 <- paste0(path, "/data/temp/domain_pc_year1112.csv")
#Created by domain1112.Rmd, which is based on domain.R and uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df1112 <- read_csv(data.path2011_2012) %>% 
  filter(
    #V1.domain < m,             # for multiple pc
    V1.domain == n,             # for n == 1
    year > 1995 & year <2013) %>%   
  left_join(lookup, by = "pc", suffix =c("","")) %>% 
  group_by(year, ladcd) %>%
  summarise(n = n()) %>%             # for n == 1  
  #summarise(n = sum(V1.domain)) %>% # for multiple pc  
  ungroup()

df <- bind_rows(df9610, df1112) %>% 
  filter(!is.na(ladcd)) %>% 
  complete(ladcd, year) %>% 
  replace(is.na(.), 0)

# tests for Scotland
# df %>% filter(substr(oa11cd, 1,1) =="S",
#               year==2010) %>% summarise(mean(n, na.rm = T))
# 
# df %>% filter(oa11cd=="S00090182")
# 
# test@data %>% filter(substr(id, 1,1) =="S",
#                      n!=0)
# 
# df %>% filter(substr(oa11cd, 1,1) =="W",
#               year==2003)
```

## Firms

```{r, eval=F}
firms <- read_csv("https://www.nomisweb.co.uk/api/v01/dataset/NM_142_1.data.csv?geography=1811939329...1811939332,1811939334...1811939336,1811939338...1811939428,1811939436...1811939442,1811939768,1811939769,1811939443...1811939497,1811939499...1811939501,1811939503,1811939505...1811939507,1811939509...1811939517,1811939519,1811939520,1811939524...1811939570,1811939575...1811939599,1811939601...1811939628,1811939630...1811939634,1811939636...1811939647,1811939649,1811939655...1811939664,1811939667...1811939680,1811939682,1811939683,1811939685,1811939687...1811939704,1811939707,1811939708,1811939710,1811939712...1811939717,1811939719,1811939720,1811939722...1811939730,1811939757...1811939767&date=latestMINUS10&industry=37748736&employment_sizeband=0&legal_status=0&measures=20100") %>% 
  dplyr::select(GEOGRAPHY_CODE, OBS_VALUE) %>% #GEOGRAPHY_NAME, 
  rename(ladcd = GEOGRAPHY_CODE,
         firms = OBS_VALUE)

df <- df %>% left_join(firms) %>% 
  mutate(n_by_firm = n/firms) %>% # N websites/firm
  #filter(!is.na(firms)) %>% 
  dplyr::select(-n, -firms) %>% 
  rename(n = n_by_firm)

# df %>% filter(is.na(n_by_firm))
# sapply(df %>% left_join(firms), function(x) sum(is.na(x)))
```

## spatial data

```{r}
# get LA for the UK
path.geo <- paste0(path, "/data/raw/Local_Authority_Districts_(December_2021)_UK_BUC.geojson")
la <- st_read(path.geo)
# source: https://geoportal.statistics.gov.uk/

# spatial transformations
la <- st_transform(la, 4326)

# cities
cities <- maps::world.cities %>% 
  filter(country.etc=="UK") %>% 
  arrange(pop) %>% tail(10) 
cities.sf <- st_as_sf(cities, coords = c("long", "lat"), crs = 4326)
```

## lisa maps

```{r}

path.out <- paste0(path, "/outputs/lisa/corrected/lisa_level_pc", n, "_la") # for unique PC
#path.out <- paste0(path, "/outputs/lisa/corrected/lisa_level_pc", m, "_la")  # for multiple PC

morani <- data.frame()

#spatial weight matrix
sf_use_s2(FALSE)
nb <- poly2nb(la)
listw <- nb2listw(nb, style = "W", zero.policy = T)

for (t in 1996:2012){
test <- la
test <- test %>% left_join(df[df$year==t,], by = c("LAD21CD"="ladcd")) %>% 
  mutate(n = ifelse(is.na(n), 0, n),           # 0 for NAs and the same for year
         year = ifelse(is.na(year), t, year))
  
#spatial weight matrix
#moved out of the loop

#Moran's I
globalMoran <- moran.test(test$n, listw, zero.policy = T, na.action = na.exclude)
a <- cbind(globalMoran$estimate[[1]],globalMoran$p.value, t[1])
morani <- rbind(a, morani)

#LISA
#lmoran <- cbind(test, localmoran(test$n, listw,  adjust.x=TRUE, zero.policy = T, na.action = na.exclude))
lmoran <- localmoran(test$n, listw,  adjust.x=TRUE, zero.policy = T, na.action = na.exclude)

quadr <- attr(lmoran, "quadr")
quadr[lmoran[,5] > 0.05 | is.na(lmoran[,5]), ] <- NA

palette_new <- grDevices::colorRampPalette(
          colors = c("blue", "lightsalmon", "lightblue", "red", "white"))(5)

ggplot(data = la, aes(fill = quadr$median)) +
  geom_sf() +
  # scale_fill_discrete_c4a_cat(name = "%", palette = "parks.charmonix",
  #                             na.translate = FALSE) +
  scale_fill_manual(values = palette_new, na.value="white", breaks = NULL, drop = F) +
  theme_minimal() +
  theme(panel.grid.major = element_line(colour = "transparent"),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank()) +
  #guides(fill = guide_legend(reverse = TRUE)) +
  labs(title = "LISA, websites per firm in a LA") + 
  theme(legend.title=element_blank()) 

theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        rect = element_blank())

filename = paste(path.out, t, ".png", sep = "")
ggsave(filename)
}
```

# Moran I

```{r}
path.out <- paste0(path, "/outputs/lisa/corrected/morani_la.csv")
morani %>% as_tibble() %>% 
  rename('morani'=V1,
                   p = V2,
                   year = V3) %>% 
  #kable() %>% 
  write_csv(path.out)
```

```{r}
path.images <- paste0(path, "/outputs/lisa/corrected/")

list.files(path = path.images, pattern = "^lisa_level_pc1_la", full.names = T) %>% 
  map(image_read) %>% # reads each path file
  image_join() %>% # joins image
  image_animate(delay=50) %>% # animates, can opt for number of loops
  image_write("corrected_lisa.gif", format = "png") # write to current dir



```

```{r}
# centers the local Moran's around the mean
lmoran$Ii <- lmoran$Ii - mean(lmoran$Ii, na.rm = TRUE) 
lmoran$lag.n<-  lag.listw(listw,lmoran$n, NAOK = TRUE)

# centers the variable of interest around its mean

lmoran$ns <- lmoran$n - mean(lmoran$n, na.rm = TRUE) 
lmoran$lag.n <- lmoran$lag.n - mean(lmoran$lag.n, na.rm = TRUE) 

signif <- 0.05
#lmoran

lmoran <- lmoran%>% 
  mutate(quadrant= ifelse(ns>0 & lag.n > 0, 1, 0)) %>% 
  mutate(quadrant= ifelse(ns<0 & lag.n < 0, 2, quadrant)) %>% 
  mutate(quadrant= ifelse(ns<0 & lag.n > 0, 3, quadrant)) %>% 
  mutate(quadrant= ifelse(ns>0 & lag.n < 0, 4, quadrant)) %>%   
  #mutate(quadrant= ifelse(lmoran$`Pr(z > 0)` > signif, 0, quadrant)) #%>% 
  mutate(quadrant= ifelse(lmoran$`Pr.z....E.Ii..` > signif, 0, quadrant)) #%>% 


#   mutate(quadrant2= ifelse(ANC_st>0 & lagANC_st > 0, 1, 0)) %>% 
# mutate(quadrant2= ifelse(ANC_st<0 & lagANC_st < 0, 2, quadrant2)) %>% 
#   mutate(quadrant2= ifelse(ANC_st<0 & lagANC_st > 0, 3, quadrant2)) %>% 
#  mutate(quadrant2= ifelse(ANC_st>0 & lagANC_st < 0, 4, quadrant2)) %>% 
#  mutate(quadrant2= ifelse(lmoran$LISA_PANC > signif, 0, quadrant2))
  
mun_merge_new<- merge(test, lmoran, by="LAD21CD")
mun_merge_new<- test %>% 
  left_join(lmoran, by="LAD21CD")

# R p value map
breaks = c(0, 1, 2, 3, 4, 5) 

cities <- maps::world.cities %>% 
  filter(country.etc=="UK") %>% 
  arrange(pop) %>% tail(10) 
cities.sf <- st_as_sf(cities, coords = c("long", "lat"), crs = 4326)


map <- tm_shape(mun_merge_new) + tm_fill(col = "quadrant", breaks = breaks, 
                                         palette = c("white","red","blue",
                                                     rgb(0,0,1,alpha=0.4),
                                                     rgb(1,0,0,alpha=0.4)), 
                                         labels = c("Not significant",
                                                    "High-High",
                                                    "Low-Low",
                                                    "Low-High",
                                                    "High-Low"), 
                                         title="LISA, websites per firm in a LA") +
  tm_legend(text.size = 1) +
  #tm_scale_bar(position = c("LEFT", "BOTTOM"),text.size = 1.0)+
  #tm_compass(type = "8star",   position = c("RIGHT", "BOTTOM"),      show.labels = 2,   text.size = 0.5)+
  #tm_borders(alpha=.1) +
  tm_borders(lwd = 0) +
  tm_shape(la, simplify = .2) + tm_borders(alpha=1, lwd = .7) +
  tm_shape(cities.sf) + tm_dots() + tm_text("name", size = .5) +
  tm_layout( frame = FALSE,  title = t) #"LISA with the R p-values ")
tmap_save(map, filename = paste(path.out, t, ".png", sep = ""))
}

#ggplot map solution.
# mun_merge_new %>% st_as_sf(crs = 27700) %>% #st_transform(4326) %>%
#   ggplot()+
#   geom_sf(aes(fill = as.factor(quadrant)))
# ggsave("test_ggplot.png", path = path)

```
