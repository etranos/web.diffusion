---
title: "s.function"
date: "`r format(Sys.time(), '%d %B, %Y, %H:%M')`"
output: 
  html_document:
    df_print: paged
    toc: true
    toc_float: true
knit: (function(inputFile, encoding) {
    rmarkdown::render(inputFile, encoding = encoding, output_dir = "../output")
  })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(broom)
library(rprojroot)
library(rgdal)
library(sf)
library(modelr)

options(scipen=10000)

# This is the project path
path <- find_rstudio_root_file()
```

## TODO

- speed, other metrics? 
- understand rsquare
- model LAs
- add SE for the line plots [link](https://stackoverflow.com/questions/26020142/adding-shade-to-r-lineplot-denotes-standard-error)

## Load data

Postcode lookup.

[source](https://geoportal.statistics.gov.uk/datasets/postcode-to-output-area-to-lower-layer-super-output-area-to-middle-layer-super-output-area-to-local-authority-district-august-2021-lookup-in-the-uk/about)

```{r}
path.lookup <- paste0(path,"/data/raw/PCD_OA_LSOA_MSOA_LAD_AUG21_UK_LU.csv")
lookup <- read_csv(path.lookup) %>% 
  select(pcds, oa11cd, lsoa11cd, msoa11cd, ladcd, ladnm) %>% 
  dplyr::rename(pc = pcds)
#glimpse(lookup)
# The problems refer to Welsh LAD names. Not a problem for the analysis.
#sapply(lookup, function(x) sum(is.na(x)))
# 10332 missing msoa11cd
```

The internet archive $1996$-$2010$ data is saved on /hdd.
The internet archive $2011$-$2012$ data is saved on ~/projects/web.diffusion/data/temp.

```{r}
n = 1 #number of unique postcodes. 
m = 12

data.folder <- "/hdd/internet_archive/archive/data/"
data.path9610 <- paste0(data.folder, "domain_pc_year.csv")
#Created by domain.R, which uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df9610 <- read_csv(data.path9610) 

data.path2011_2012 <- paste0(path, "/data/temp/domain_pc_year1112.csv")
#Created by domain1112.Rmd, which is based on domain.R and uses domain instead of host.
#This is what we use for the hyperlinks paper as per George's script

df1112 <- read_csv(data.path2011_2012)

df <- bind_rows(df9610, df1112) %>% 
  filter(
    V1.domain < m,             # for multiple pc
    #V1.domain == n,             # for n == 1
    year > 1995 & year <2013) %>%   
  left_join(lookup, by = "pc", suffix =c("","")) %>% 
  group_by(year, ladcd) %>%
  summarise(n = n()) %>%            # for multiple pc  
  #summarise(n = sum(V1.domain)) %>% # for n == 1 
  ungroup()


# tests for Scotland
# df %>% filter(substr(oa11cd, 1,1) =="S",
#               year==2010) %>% summarise(mean(n, na.rm = T))
# 
# df %>% filter(oa11cd=="S00090182")
# 
# test@data %>% filter(substr(id, 1,1) =="S",
#                      n!=0)
# 
# df %>% filter(substr(oa11cd, 1,1) =="W",
#               year==2003)
```

## Help, not really used

```{r, eval=FALSE}

df.test <- df %>% filter(oa11cd == "E00000007" |
                         oa11cd == "E00000021" |
                         oa11cd == "E00000025") %>% 
  mutate(rownum = row_number(),
         value2 = n/max(n))

# function to select unique groups
sample_n_groups = function(grouped_df, size, replace = FALSE, weight=NULL) {
  grp_var <- grouped_df %>% 
    groups %>%
    unlist %>% 
    as.character
  random_grp <- grouped_df %>% 
    summarise() %>% 
    sample_n(size, replace, weight) %>% 
    mutate(unique_id = 1:NROW(.))
  grouped_df %>% 
    right_join(random_grp, by=grp_var) %>% 
    group_by_(grp_var) 
}

df.test <- df %>% group_by(oa11cd) %>% 
  sample_n_groups(10000) %>% 
  arrange(oa11cd, year) %>% 
  select(-unique_id)

fit <- nls(n ~ SSlogis(year, Asym, xmid, scal), data = df.test)
  
fd.nls <- df.test %>% 
  group_by(oa11cd) %>%
  select(year, oa11cd, n) %>% 
  do(fitloop = tidy(nls(n ~ SSlogis(year, Asym, xmid, scal), data = .))) %>% 
  unnest(fitloop)

fd.nls %>% filter(term=="xmid") %>% 
  summarise(rng = range(estimate))
```

## Test for S function estimation

To interpret the coefficients, check [SSlogis](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/SSlogis).
This is a self-starting model, which estimates the starting values. 


```{r}
df.test <- df %>% filter(ladcd == "E06000002") 

fit <- nls(n ~ SSlogis(year, Asym, xmid, scal), data = df.test)
summary(fit)

df.test$prednls = predict(fit)

ggplot(df.test, aes(x = year, y = n) ) +
  geom_point() +
  geom_line(aes(y = prednls), size = 1) +
  scale_x_continuous("year", labels = as.character(df.test$year), breaks = df.test$year) +
  ylab("N. of websites")
  #scale_x_date(breaks = df.test$year) 
```

## S function estimation for all the country

```{r}
df.country <- df %>% group_by(year) %>% 
  summarise(n = sum(n))

fit <- nls(n ~ SSlogis(year, Asym, xmid, scal), data = df.country)
summary(fit)

rsquare(fit, df.country)

df.country$prednls = predict(fit)

df.country %>% 
  ggplot(aes(x = year, y = n) ) +
  geom_point() +
  geom_line(aes(y = prednls), size = 1) +
  scale_x_continuous("year", labels = as.character(df.country$year), breaks = df.country$year) +
  geom_vline(xintercept = 2003) +
  ylab("N. of websites") +
  ggtitle("Country")
  #scale_x_date(breaks = df.test$year) 
```

## Loop for S function estimation for Local Authorities

```{r}
a.la <- as_tibble()
all.la <- df %>% filter(ladcd != "NA") %>% distinct(ladcd) %>% simplify2array()
simplify2array(all.la)

for (i in all.la){
  data <- df %>% filter(ladcd==i) %>%
    select(year, ladcd, n) 
    #lm(n~year, data = .)
    #tryCatch(tidy(nls(n ~ SSlogis(year, Asym, xmid, scal), data = .)), , error=function(e) NULL)
  tryCatch(b.la <- tidy(nls(n ~ SSlogis(year, Asym, xmid, scal), data = data)), error=function(e) NULL)
  tryCatch(a.la <-rbind(a.la,b.la), error=function(e) NULL)
}

a.la %>% filter(term=="xmid") %>% 
  mutate(estimate = round(estimate, 0)) %>% 
  select(estimate) %>% 
  summarise(r = range(estimate),
            median = median(estimate))

a.la %>% filter(term=="xmid") %>% 
  mutate(estimate = round(estimate, 0)) %>% 
  filter(p.value < 0.01) %>%    # (227759 - 212782)/227759  # 6% not sig 
  select(estimate) %>% 
  ggplot(aes(x=estimate)) + 
  geom_histogram(binwidth = 1) +
  geom_vline(xintercept = 2003, colour = "red")
  scale_x_continuous("year", labels = round(a$estimate, 0), breaks = round(a$estimate, 0)) 
  #scale_x_discrete(labels = as.factor(a$estimate))
  
a.la <- a.la %>% filter(term=="xmid") %>% 
  bind_cols(as.tibble(all.la)) %>% 
  mutate(estimate = round(estimate, 0)) %>% 
  filter(p.value < 0.01) %>%    # 376 / 376, all sig
  select(ladcd, estimate)
  #filter(estimate>2011)
```

## Loop for S function estimation for Local Authorities keeping the model objects

```{r}
a.la <- as_tibble()
all.la <- df %>% filter(ladcd != "NA") %>% distinct(ladcd) %>% simplify2array()
simplify2array(all.la)

for (i in all.la){
  data <- df %>% filter(ladcd==i) %>%
    select(year, ladcd, n) 
    #lm(n~year, data = .)
    #tryCatch(tidy(nls(n ~ SSlogis(year, Asym, xmid, scal), data = .)), , error=function(e) NULL)
  tryCatch(model <- (nls(n ~ SSlogis(year, Asym, xmid, scal), data = data)), error=function(e) NULL)
  model.name <- paste0("model.", i)
  assign(model.name, model)
  tryCatch(b.la <- tidy(model), error=function(e) NULL)
  
  tryCatch(b.la$r2 <- rsquare(model, data), error=function(e) NULL)
  
  tryCatch(a.la <-rbind(a.la,b.la), error=function(e) NULL)
}

a.la <- a.la %>% filter(term=="xmid") %>% 
  bind_cols(as.tibble(all.la)) %>% 
  #mutate(estimate = round(estimate, 0)) %>% 
  #summarise(r = range(r2))
  filter(p.value < 0.01 & 
         r2 > 0.9) %>%    # 376 / 376, all sig, 330 r2 > 0.9
  select(ladcd, estimate, r2, std.error) %>%
  mutate(fast = ifelse(estimate < 2003, "fast", "slow")) 


# predictions 
pred <- tibble()
help <- tibble(year = 1996:2012)
for (i in all.la){
  model.name <- (paste0("model.", i))
  #assign(col.name, paste0("pred.", i))
  pred_ <- df %>% filter(ladcd == i) %>% 
    mutate(prednls = eval(call("predict", as.name(model.name))))  # "pred.{i}" :=
  help_ <- help %>% left_join(pred_)  

  pred <- pred %>% bind_rows(help_) 
}

# plots
path.out <- paste0(path, "/outputs/lad_")

for (i in all.la){
p <- ggplot(pred %>% left_join(a.la) %>% 
              filter(ladcd == i), aes(x = year, y = n, color = fast) ) +
  geom_point() +
  geom_line(aes(y = prednls), size = 1) +
  scale_x_continuous("year", labels = as.character(df.test$year), breaks = df.test$year) +
  geom_vline(xintercept = pred %>% left_join(a.la) %>% filter(ladcd == i) %>% select(estimate) %>% first()) +
  ylab("N. of websites") + 
  labs(title = paste0(la@data %>% filter(LAD21CD== i) %>% select(LAD21NM) %>% first(), ", ", i)) +
  theme(legend.position=c(.9,.15)) +
  scale_color_manual(values = c("fast" = "purple",
                                "slow"="orange")) 

ggsave(p, filename = paste(path.out, i, ".png", sep = ""))
  
}
```

## Xmid frequency graph

```{r}
ggplot(a.la, aes(as.factor(round(estimate, 0)))) +
  geom_bar(fill = "#0073C2FF")
```

write the object with all the models

```{r eval=F}
path.out <- paste0(path, "/data/temp/s_la.csv")
write_csv(a.la, path.out)
```

## Survival

- pop.density
- income
- firms
- employment
- N/S
- London

Time series formula will look like this:

`coxph(Surv(tstart, tstop, infect) ~ treat + inherit + steroids, data =newcgd, cluster = id)`

This is what @perkins2005international should be using.
See p. 7 [here](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf).


```{r}
library(survival)

test.surv.data <- a.la %>% mutate(status = 1) %>% 
  left_join(data.la, by = c("ladcd" = "LAD21CD")) %>% 
  left_join(df %>% filter(year == 2012), by = "ladcd")

cf <- coxph(Surv((estimate-1996), status) ~ log(distLondon) + log(distMet) + log(pop16_64) + log(pop.dens) + log(econ.act) + log(earnings) + log(total.busi) + south + log(n), data=test.surv.data, weights = 1/(std.error)^2)
summary(cf)

```

## Map the xmid

Spatial data

```{r}
# get LA for the UK
path.geo <- paste0(path, "/data/raw/Local_Authority_Districts_(December_2021)_UK_BUC.geojson")
la <- readOGR(path.geo)
# source: https://geoportal.statistics.gov.uk/

# spatial transformations
la <- spTransform(la, CRS("+init=epsg:4326"))

la.f <- fortify(la, region = "LAD21CD")

# cities
cities <- maps::world.cities %>% 
  filter(country.etc=="UK") %>% 
  arrange(pop) %>% tail(10) 
tif_sf <- st_as_sf(cities, coords = c("long", "lat"), crs = 4326)
```

```{r}

la.f <- la.f %>% left_join(a.la, by = c("id" = "ladcd")) %>% 
  mutate(fast = ifelse(estimate < 2003, "fast", "slow")) %>% 
  arrange(order)  # if i don't order merge.nuts.f loses order and the map has gaps


# oa.gb.f %>% mutate(fast = ifelse(estimate < 2003, 1, 0)) %>% 
#   arrange(order) %>% 
ggplot(data = la.f, aes(x = long, y = lat)) +
  geom_polygon(aes(group = group, fill = estimate)) + #fill = fast, 
  #theme_nothing(legend = TRUE) +
  labs(title = "Clusters") +
  scale_fill_continuous(type = "viridis") +
  theme_void()

ggplot(data = la.f, aes(x = long, y = lat)) +
  geom_polygon(aes(group = group, fill = fast)) + #fill = fast, 
  #theme_nothing(legend = TRUE) +
  labs(title = "Clusters") +
  geom_point(data = cities, aes(x=long, y=lat), colour = "black", size = .1) + 
    geom_text(size = 2, check_overlap = F, nudge_y = .2, data = cities, aes(x=long, y=lat, label = name)) +
  #scale_fill_continuous(type = "viridis") +
  theme_void()

```

```{r}
la.f %>% left_join(df %>% filter(year==2012), by = c("id" = "ladcd")) %>% 
  arrange(order) %>% # if i don't order merge.nuts.f loses order and the map has gaps
  ggplot(aes(x = long, y = lat)) +
  geom_polygon(aes(group = group, fill = n)) + #fill = fast, 
  #theme_nothing(legend = TRUE) +
  labs(title = "Clusters") +
  scale_fill_continuous(type = "viridis") +
  theme_void()

```

## time series clusters

2 clusters is the preferred solution, but when I plot individual local authorities 
I cannot interpret the differences between the two clusters

```{r}
library(dtwclust)
library(tsbox)
library(imputeTS)


# turn to a wide format
ts.wide <- df %>% 
  filter(!is.na(ladcd)) %>% 
  rename(time = year) %>%
  dplyr::select(time, ladcd, n) %>% 
  #mutate(year = )
  ts_wide()

# impute with means of each TS
# TODO check sensitivity
ts.wide <- na_mean(ts.wide)
#sapply(ts.wide, function(x) sum(is.na(x)))

# extract names
ts.wide.names <- names(ts.wide[,-1])
ts.wide <- ts.wide[,-1]

# standardise
ts.wide <- zscore(ts.wide)

# convert to list for tsclust
ts.wide <- split(ts.wide, rep(1:ncol(ts.wide), each = nrow(ts.wide)))

# 5 - 20 cluster sollutions
pc_k <- tsclust(ts.wide, k = c(2:10), seed = 94L,
                type="partitional",
                distance = "dtw_basic", centroid = "pam", trace = T, 
                args = tsclust_args(dist = list(window.size = 20L)))

# decide k, see RJ-2019-023 for max/min
#names(pc_k) <- paste0("k_", c("5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15"))
names(pc_k) <- paste0("k_", c("2", "3", "4", "5", "6", "7", "8", "9", "10"))
cvis <- sapply(pc_k, cvi, type = "internal")
cvis
# max Sil
# max SF
# max CH
# min DB
# min DBstar
# max D
# min COP

# loop for k decision
pref.cvi <- NULL

max.cvis <- c(1:3,6)
for (i in max.cvis){
  a <- which.max(cvis[i,])
  pref.cvi <- c(pref.cvi, a)
}

min.cvis <- c(4:5,7)
for (i in min.cvis){
  a <- which.min(cvis[i,])
  pref.cvi <- c(pref.cvi, a)
}

print(pref.cvi) # just count how many time each k_* appears, this is the preferred solution 

pc_k <- tsclust(ts.wide, k = 7, seed = 94L,  # 7 or 2
                type="partitional", 
                distance = "dtw_basic", centroid = "pam", trace = T, 
                args = tsclust_args(dist = list(window.size = 20L)))
plot(pc_k)

clusters <- data.frame(LAD=ts.wide.names,
                           cluster=pc_k@cluster)

```

## Mapping the clusters

```{r include=TRUE, echo=FALSE, results= 'markup', message=FALSE, warning = FALSE, fig.height=10, fig.width=10}

la.f <- la.f %>% left_join(clusters, by = c("id" = "LAD")) %>% 
  arrange(order)  # if i don't order merge.nuts.f loses order and the map has gaps

ggplot(la.f %>% filter(!is.na(r2)), aes(x = long, y = lat)) +
  geom_polygon(aes( group = group, fill = as.factor(cluster))) +
  #theme_nothing(legend = TRUE) +
  labs(title = "Clusters") +
  scale_fill_viridis_d()+
  theme_void()

# ggplot(la.f, aes(x = long, y = lat)) +
#   geom_polygon(aes( group = group, fill = as.factor(cluster.down))) +
#   theme_nothing(legend = TRUE) +
#   labs(title = "Download clusters") +
#   scale_fill_viridis_d()+
#   theme_void()
# 
# cluster.upL <- c(6, 9, 11, 12, 13)
# cluster.upS <- c(1, 2, 3, 7, 8, 10)
#  
# la.f_ <- la.f #plot the background
# 
# cluster.upL.plot <- la.f %>% 
#   filter(cluster.up %in% cluster.upL) %>% 
#   ggplot(aes(x = long, y = lat)) +
#   geom_polygon(data = la.f_, color='grey', fill='white', aes(x=long, y=lat, group=group, colour = 'grey30')) +
#   geom_polygon(aes( group = group, fill = as.factor(cluster.up))) +
#   labs(title = "Larger clusters, upload speeds") + 
#   scale_fill_viridis_d() +
#   guides(fill=guide_legend(title="Clusters")) +
#   theme_void()
# 
# cluster.upS.plot <- la.f %>% 
#   filter(cluster.up %in% cluster.upS) %>% 
#   ggplot(aes(x = long, y = lat)) +
#   geom_polygon(data = la.f_, color='grey', fill='white', aes(x=long, y=lat, group=group, colour = 'grey30')) +
#   geom_polygon(aes( group = group, fill = as.factor(cluster.up))) +
#   labs(title = "Smaller clusters, upload speeds") + 
#   scale_fill_viridis_d() +
#   guides(fill=guide_legend(title="Clusters")) +
#   theme_void()
# 
# plot_grid(cluster.upL.plot, cluster.upS.plot, nrow = 1,
#           align = "h", axis = "l", 
#           #labels = c(".co.uk / pop., 2000", ".co.uk / pop., 2010", "non .co.uk / pop., 2000", "non .co.uk / pop., 2010", ""), 
#           #label_size = 12,
#           greedy = T)
#           #rel_heights = c(1, 1, .3)) # this is the relative position of the 3 lines
# 
# #ggsave("all_maps2.png") # this is the colour map for the preprint
# out.path.map <- paste0(path, "/paper/v1/figures/map.up.clusters.png")
# ggsave(out.path.map)#,
#        #width = 210, #A4
#        #height = 297, #A4
#        #units = "mm")
```