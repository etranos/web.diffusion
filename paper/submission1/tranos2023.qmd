---
title: "A multi-scale story of the diffusion of a new technology: the web"
author:
  - name: Emmanouil Tranos
    email: e.tranos@bristol.ac.uk
    affiliations: 
      - id: some-tech
        name: University of Bristol and The Alan Turing Institute
        department: 
        address: 
        city: 
        state: State
        postal-code: UK
    corresponding: e.tranos@bristol.ac.uk
address:
  - attributes:
        corresponding: true
    note: This is the first author footnote.
abstract: |
  This paper maps the participation in the digital economy and its evolution in the UK over space and time. Most of the existing economic geography literature which dealt with the spatiality of the internet employed supply-side measures, such as infrastructural capacity, in order to understand the geography of the digital economy and its potential spatial economic effects. Useful as these approaches might have been, they cannot capture the micro-processes and the characteristics of the individual online behaviour. Using large volumes of archived and geolocated web content, this paper models the diffusion of web technologies over space and time in the UK. The data and geolocation strategy allow to capture these processes at a very granular spatial scale. The modelling approach, which is based on simple spatial analytical methods and on the estimation of diffusion curves at various scales, enables to depict the role of geography and other cognitive factors which drove the diffusion of web technologies. Although the focus is on a recent historical period -- 1996-2012 -- the results of the analysis depict diffusion mechanisms which can be very useful in understanding the evolutionary patterns of the adoption of other newer technologies.
keywords: 
  - keyword1
  - keyword2
date: last-modified
bibliography: bibliography
format:
  elsevier-pdf:
    keep-tex: true
    text: |
      \usepackage[demo]{graphicx} % "demo" option just for this example
      \usepackage{subcaption}
    journal:
      name: Journal Name
      formatting: preprint
      model: 3p
      cite-style: authoryear
editor: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE) # By default, hide code; set to TRUE to see code
#knitr::opts_chunk$set(fig.pos = 'p') # Places figures on their own pages
knitr::opts_chunk$set(out.width = '100%', dpi=300) # Figure resolution and size
knitr::opts_chunk$set(fig.env="figure") # Latex figure environment

#insert libraries here
library(rprojroot)
library(tidyverse)
library(knitr)
library(kableExtra)
library(sf)
library(cowplot)

# This is the project path
path <- find_rstudio_root_file()
```


# Introduction {#sec-introduction}

<!-- geographers lack of interest + motivation -->

Geographers were always interested in how new technologies and
innovations diffuse across space and time and, importantly, how such
spatio-temporal processes can be modeled. After all, diffusion together
with invention and innovation are considered the pillars of
technological change [@das2022diffusion]. The seminal contribution of
@hagerstrand1968innovation is illustrative of this early interest.
However, the torch of exploring and modelling such processes had been
passed to other disciplines such as economics, business studies and
sociology well before the 'cultural turn' of economic geography
[@perkins2005international]. A potential explanation of the lack of
geographical studies exploring the diffusion of new and, more
specifically for this paper, digital technologies across both space
and time can be attributed to the scarcity of relevant and granular
enough data. As @zook2022mapping highlight, digital activities are
hardly ever captured in official data.

<!-- contribution -->

This paper offers such a contribution: a geographical study illustrating
how a new technology that is the web diffused over space and time in the
UK at a high level of spatial granularity during the $1996-2012$ period.
It does so by employing a novel source of big data which captures the
active engagement with web technologies during that period. By
addressing this empirical question this paper exemplifies how the
combination of data sources which escape the traditional social science
domain and adequate research methods can offer new lenses to
geographical research regarding the understanding of technological
diffusion.

**SAY SOMETHING ABOUT SPATIAL ANALYTICS, DIST PAPER BY ARP, NOT SPECIFIC EXPLANATORY VARIABLES, INTANGIBLE**

<!-- motivation -->

The motivation for this paper lies in the fact that there are various
stakeholders who are interested in knowing how new digital technologies
diffused over space and time and use this knowledge to make predictions
regarding the diffusion of related *future* technologies. As per
@leibowicz2016representing, historical studies agree that technologies diffuse 
differently in terms of times, rates, and geographies and can be driven
by related policies [@victor1993]. @meade2021modelling highlight
that a variety of actors have a direct interest in gaining such
knowledge including network equipment suppliers; network operators,
regulatory and local authorities. These processes and their effects vary
a lot across scales: although the diffusion of a new technology might
not be optimal at a local level, it might be beneficial from a global
perspective as it could lead to faster diffusion to less advantaged
places [@leibowicz2016representing]. Despite the spatial heterogeneity
of such diffusion mechanisms and the policy relevance, there are very
limited attempts in the literature to analyse the diffusion of new
digital technologies at a detailed geographical level.

<!-- diffusion and adoption -->

Technological diffusion, which is by definition an aggregated process,
can be discussed in parallel with individual adoption mechanisms. On the
one hand, @rogers2010diffusion identifies early adopter of new
technologies as 'knowledgeable risk takers' and @griliches1957 as
'profit maximisers' [@ding2010modeling]. Such individual agents are
rewarded because of their attitude towards new technologies and
innovations. On the other hand, @perkins2011internet attribute diffusion
to two processes: (i) epidemic-like mechanisms, which are governed by
distance, proximity and social interactions, and (ii) economic
mechanisms as new innovations are adopted by users as they become more
profitable, valueable and useful.
<!-- The criticisms are mainly based on the fact that although the approach -->
<!-- gives an idea of aggregate (industry or household) behaviour, it does -->
<!-- not focus on the individual's (firm or household) adoption process -->

<!-- data and early engagement with the web -->

This paper focuses on the diffusion of the web as new technology during
the $1996-2012$ period. This was an exciting period for digital
technologies as it corresponds with the commercialisation of the
internet and, consequently, its almost universal adoption. The reader is
reminded that it was only in 1994 when Netscape Navigator was
introduced, a year before Microsoft's Internet Explorer.[^1] Also, only
$9$ per cent of UK's households had access to the internet in $1998$
[@ons2018], the web included mostly static webpages, there were no
social media and web browsing was happening exclusively from desktop PCs
as there were no smartphones [@tranosuk]. Hence, it is fair to say that
the study period captures the very early stages of the diffusion on a
new technology that is the web as well as its maturity. 
The former is a key point in the lifecycle of a
new technology. Firstly, during this period new technologies are
expensive, crude and imperfect [@rosenberg1994exploring; @wilson201281].
A simple comparison between Web 1.0 and Web 3.0 applications clearly
illustrates this [@tranos2020social]; for instance a static website compared 
with a platform like `github`, which enables cooperation between users and 
the creation of new information, meaning, and knowledge 
[@faraj2016special; @barassi2012does]. 
During this period the performance
of a new technology is the main attraction and not the cost to access
and use it [@wilson2011lessons]. There is a broader theoretical
discussion in the literature regarding the motivation behind early
adoption. As summarised by @perkins2005international, on the one hand,
epidemic models highlight the role of interpersonal contacts as a way
for new technologies to diffuse. On the other hand, economic models
underline the importance of heterogeneity. Different firms have
different structures and business plans, which define the potential
economic returns of the adoption of a new technology and, therefore, the
choice to adopt a new technology becomes an individual option. From a
broader and evolutionary perspective, initial conditions are essential
for the creation and evolution of path-dependent technological
development trajectories [@neffke2011regions; @simmie2014new]. This
argument is even more relevant when the focus is on digital technologies because 
of the commonly found lag between investment and economic returns as reflected in 
the Solow paradox [@acemoglu2014return; @brynjolfsson2018artificial].

[^1]: <https://www.theguardian.com/global/2015/mar/22/web-browser-came-back-haunt-microsoft>

Importantly, the data used here depicts the *active* engagement with the
web in the UK as it contains all commercial websites that 
(i) are part of the UK's relevant second level domain (SLD, .co.uk), 
(ii) have been archived by the Internet Archive[^2], and 
(iii) include a mention to at least one valid UK
postcode in the web text. 
The act of creating a website is understood here as active engagement with the
web vis-Ã -vis the more passive act of browsing the web or having an
internet connection [@tranosuk]. Previous studies have focused mostly on
more passive notions of engaging with digital technologies such as
internet adoption and internet speeds [e.g. @blank2018local;
@riddlesden2014broadband]. More details about the data and the data
generation process can be found in @sec-datamethods. 

**S curve??**

<!-- description of the methods and results -->

<!-- contents -->

@grubler1990rise Later Hagerstrand conceptualized physical "barrier"
effects like lakes or uninhabited areas, which, in addition to distance,
act as further retarding effects on diffusion. These are formalized in
the form of "zero" or "half" contact multiplicators on the (distance
decaying) message flows.

@grubler1990rise With respect to the formalization of the communication
flows Hagerstrand defines a "mean information field" (MIF), in which the
probability of communication is a negative function of distance between
individuals

@wilson201281 Logistic growth describes an initial period of gradual
diffusion as a technology is introduced as a new commercial application,
moving then through a rapid, exponential growth phase, before slowing
and eventually saturating [@grubler1999dynamics]. The substitution of
incumbent technologies by new competitors leads to subsequent decline
and eventual obsolescence.

# Literature review {#sec-litreview}

<!-- key concepts -- copied from help.Rmd -->

Geographical diffusion is a synthesis of different processes. On the one
hand, a purely spatial or, in other words, contagious processes can be identified. 
Adjacency and, more broadly, distance are the key drivers of
diffusion. This perspective draws similarities with epidemics:
innovation just like pathogens spreads because of contagion and,
consequently, proximity and exposure [@hivner2003facilitating]. On the
other hand, there is a hierarchical processes. Instead of
horizontal distance-based diffusion mechanisms, the top-down hierarchy
of urban systems shapes technological diffusion. In reality, the synthesis of these
two processes represents how new technologies diffuse over space and
time [@morrill2020spatial].

These ideas were firstly introduced by Torsten HÃ¤gerstrand and his
thesis entitled 'Innovation Diffusion as a Spatial Process'
[@hagerstrand1968innovation]. HÃ¤gerstrand was the first one to identify
diffusion as a geographical process. The starting point was the idea
that diffusion is based on passing information through social networks,
which themselves tend to be defined by geography. Hence, he identified
the 'neighbourhood' effect of how information, and consequently,
innovation diffuse. He used agricultural innovations to test and model
his ideas using Monte Carlo simulations. HÃ¤gerstrand also incorporated
the role of hierarchy and how some phenomena maybe firstly adopted in
larger cities and then diffuse to second tier ones. This is a sequential
instead of a simultaneous process, which resembles the 'lead-lag'
spatial acceleration effect in market research [@bento2018time;
@PERES201091]. HÃ¤gerstrand is more widely known though for highlighting
the role time plays in the diffusion of innovations: an early-pioneering
period, a middle fast accelerating period and a final saturation one
[@morrill2020spatial].

The temporal dimension was further explored by Everett Rogers and his
seminal work on 'Diffusion of Innovations' [@rogers2010diffusion].
Rogers being a sociologist focused not on the diffusion of
innovations over space and time, but instead on the adoption of new
technologies and innovations by individuals and the individual
mechanisms that drive the decisions behind adoption. He identified five
groups of individuals regarding their adoption speed: innovators, early
adopters, early majority, late majority and laggards. The key mechanism of
diffusion and adoption is communication and how knowledge is
transferred within a social system. Therefore, all approaches agree on
the S-shaped diffusion and cumulative adoption pattern
[@grubler1990rise].

Schmidt's Law empirically illustrates a similar pattern. *Core* and
usually highly agglomerated regions is where new technologies are
invented and commercially deployed [@grubler1990rise].
This is where the first adopters tend to be based. Then, technologies
spread to the *rim* and eventually to the *periphery*. Although adoption
pace might be higher when new technologies finally arrive to the
periphery, the saturation levels there may never reach the ones in the
core because of the lack of infrastructure or other necessary
institutions [@leibowicz2016representing]. @grubler1990rise effectively
summarises the three key characteristics of the spatial diffusion
process: (i) the cumulative level of adoption follows an S-shaped
pattern just like purely temporal models; (ii) diffusion is shaped by a
hierarchy effect in a form of a centrifugal force: from core to
periphery; and (iii) diffusion is also shaped by distance and a
neighbourhood effect and contaminate nearby locations. These are the three
mechanisms that the empirical analysis in @sec-results investigates.

<!-- previous studies -->

The remaining of this section reviews empirical studies which analysed
the diffusion on new technologies over space and time. Although the
spatial dimension is present in most of the following studies, the level
of spatial detail is always more coarse than the one adopted
here. @beardsell1999spatial studied the evolution of the computer
industry in $317$ US metro areas during the $1977-1992$ period using
employment data. Their analysis indicated that the relative size
distribution holds for urban computer employment and also urban
heterogeneity is essential in explaining this distribution. In a recent
study, @bednarz2020pulled focused on wind turbines and modelled their
spatial diffusion across $402$ German regions during $1970-2015$. Their
key finding is that local demand than local supply was the main driving
factor.

At a global scale @perkins2005international explored whther the
diffusion rate of new technologies is driven by a latecomer advantage
and the engagement with the global economy via foreign direct
investments and trade. Their results illustrate that indeed latecomers
and developing countries experience diffusion of new technologies more
rapidly than early adopters and developed countries. At the same scale,
@perkins2011internet explored whether the adoption of previous
communication technologies that is mail, telegrams and telephones was
shaped by similar socioeconomic factors as the internet. Their results
indicated common patterns regarding the drivers behind the adoption of
different communication technologies.

Turning to studies that share more technological and scalar similarities
with this paper, @ding2010modeling modelled the spatial diffusion of
mobile telecommunications across regions in China. Their analysis
indicated that socioeconomic characteristics are important determinants
of the timing, speed and the level of mobile diffusion within China.
Using data from a Hungarian online social network, @lengyel2020role
analysed its adoption and the churn at a very granular spatial level.
Their results are in agreement with early theoretical and empirical
contributions reviewed here: assortativity, urban scaling and distance
are the key drivers of spatial diffusion. At a global scale
@PAPAGIANNIDIS2015308 modelled the diffusion of different web
technologies technologies and practices. Interestingly, they did so by
using similar, but less extensive data as the one used here. Their
analysis illustrated how the diffusion of different web technologies and
practices follow an S-shaped pattern as well as the different diffusion
rates of the different technologies and practices.

All in all, ...

<!-- @fritsch2015new analyze the spatial diffusion of laser technology -->
<!-- research in West Germany from 1960, when this technology began, until -->
<!-- 2005. -->
<!-- in one region  -->
<!-- @leibowicz2016representing for energy There is no generally accepted -->
<!-- theory that explains diffusion rate heterogeneity across technologies, -->
<!-- but several factors are considered important. Greater unit scale and -->
<!-- larger market size contribute to slower diffusion. Requirements for -->
<!-- interrelated technologies or complex infrastructures also hinder the -->
<!-- diffusion process (Grubler, 2012). -->

<!-- Mobile phones benefited from early deployment in recreational boats and -->
<!-- automobiles, where the traditional competitor was not a viable option. -->
<!-- In the early stages of diffusion, performance is a more important driver -->
<!-- of adoption than cost competitiveness. Typically, significant cost -->
<!-- reductions only occur once the technology reaches a deployment level -->
<!-- capable of supporting standardization and mass production (Wilson, -->
<!-- 2012). -->

<!-- @leibowicz2016representing Empirical evidence supports the validity of -->
<!-- Schmidt's Law over a wide range of technologies, time periods, and -->
<!-- geographical contexts. A recent meta-analysis of technology up-scaling -->
<!-- found that diffusion accelerated moving from the core to the rim and -->
<!-- periphery for technologies as diverse as natural gas power, oil -->
<!-- refineries, and automobiles (Wilson, 2009). One historical example that -->
<!-- conforms particularly well to Schmidt's Law is the diffusion of coal -->
<!-- power in Europe (Grubler, 2012). England emerged as the core region for -->
<!-- coal power because it had legal and economic institutions that -->
<!-- incentivized scientific pursuits, domestic coal reserves, and a clear -->

<!-- industrial motivation to replace water power with coal. ... -->

<!-- country level, not sure if needed. -->

<!-- @bento2018time explore What determines the duration of formative phases -->

<!-- for energy innovations in different markets? We are interested both in -->

<!-- initial markets (also: core, lead, first mover, early adopter) where -->

<!-- formative phases prepare technologies for mass commercialization, and in -->

<!-- follower markets (also: periphery, lag, late adopter) where accelerated -->

<!-- formative phases may benefit from diffusion and spillovers. -->

# Data and Methods {#sec-datamethods}

**ADD FAMILY OF WEB TECHNOLOGIES**

<!-- main metric -->

To capture the diffusion of web technologies, a website density metric is developed 
for two different geographical scales: the Local Authority Districts (LAD) and the 
Output Areas (OA). The former is an administrative unit and there are 
c. 374 such units in the UK. The latter is a census-based geographical unit, which 
is very small as there are c. 230,000 of them in the UK. This methodological choice 
will allow the mapping of the diffusion of web technologies and the assessment of 
the diffusion mechanisms at these two very different spatial scales.

<!-- web data -->

The counts of websites at these scales are calculated using data from the 
Internet Archive [^2] and, specifically, the JISC UK Web Domain Dataset [@ukwebarchive]. 
The Internet Archive is one of the most complete and oldest archive of webpages 
in the world operating since 1996 [@ainsworth2011much; @holzmann2016dawn].
It is a web crawler, which discovers webpages by following the hyperlinks
of every webpage its archives.
This dataset, which is curated by the British Library, contains all
the archived webpages from the UK ccTLD (.uk) from the 1996â2012 period. 
In essence, this is a long list of 2.5 billion URLs of archived webpages including 
also the archival timestamp. 

[^2]: [https://archive.org/](See%20https://archive.org/).

Instead of using the whole .uk country code top-level domain (ccTLD), this 
paper focuses on its commercial subset, the .co.uk SLD. 
This choice decrease the heterogeneity of the web data as 
such commercial websites have rather specific aims: they are used to diffuse information,
support online transactions and share opinions [@THELWALL2000441; @blazquez2018big]. 
Although a UK company can adopt a generic TLD such as .com and these cases escape 
the data used here, such omissions should not affect our results given the
popularity of the .uk ccTLD [@tranosuk]: UK consumers prefer to visit a .uk website
when they are searching for products or services [@hope]; and
anecdotal evidence indicates that during the first half of 2000, three
.co.uk domains were registered every minute [@oecd_coms]. 
Importantly, previous studies illustrated that .co.uk is the most popular UK SLD
[@tranosuk].

The text from these webpages was scanned using a regular expression (regex) to
identify strings of text which resemble UK postcodes and one fifth of them included
a mention to a postcode [@BL2013geo]. This information allows the geolocation of the 
data and the creation of the LAD and OA counts. 

The data cleaning process included an aggregation step, through which
the archived webpages were aggregated to the parent websites. This website 
reconstruction allows the creation of a *website* instead of a *webpage* density 
metric. Websites, which are hierarchical structures, tend to represent specific 
orgnanisations or entities, and, arguably, are more meaningful observational units 
than webpages, which ignore the upstream dependencies. 
Based on the following example, all three webpages are part of the 
same website ([http://www.website.co.uk](http://www.website.co.uk)) and at the end 
only websites and not the nested webpages were considered as otherwise the metric 
would have been biased towards large, place specific, websites. 

  - [http://www.website.co.uk/webpage_a](http://www.website.co.uk/webpage_a) B15 2TT

  - [http://www.website.co.uk/webpage_b](http://www.website.co.uk/webpage_b) BS8 1TH

  - [http://www.website.co.uk/webpage_c](http://www.website.co.uk/webpage_c) B15 2TT

\noindent What is challenging is that this aggregation approach,
which has been used elsewhere [@tranosuk; @shoreditch] may lead to websites with 
multiple postcodes. As per the above example, [www.website.co.uk](http://www.website.co.uk)
includes two unique postcodes: B15 2TT and BS8 1SS. 
The distribution of postcodes for 2000 is presented in Table \ref{f2000}, which 
clearly illustrates the wide range. At the left end of the distribution, there are 
websites anchored to a unique location (72% of all the reconstructed websites in
2000), which may represent a small company with a single trading location. At the 
right end, we have websites with thousands of different postcodes.
Considering the time period of the analysis, such cases can represent directories
which used to be popular in the pre-search engines early times of the commercial 
internet [@tranosuk].[^3]

The analysis presented here is based on two subsets of these data. Firstly, on websites, 
which only contain only one unique postcode and, therefore, the geolocation process 
does not suffer from noise. As a robustness check, the analysis is replicated 
for an extended subset of websites, which include up to ten unique postcodes. These 
websites are geolocated by equally attaching them to multiple locations. This extended 
sample includes 94 percent of all the archived websites in 2000.

[^3]: See Figure A1 in **Appendix A in the supplemental data online for examples**.

```{r eval=T, echo=FALSE, results='asis'}

# load 2000 co.uk
# path.2000 <- "C:/Users/nw19521/OneDrive - University of Bristol/projects/archive/nuts/all2000couk.csv"
# path.2000 <- "/Users/nw19521/Library/CloudStorage/OneDrive-UniversityofBristol/projects/archive/nuts/all2000couk.csv"
path.2000 <- paste0(path, "/data/temp/all2000couk.csv")

all2000.duplicates <- data.table::fread(path.2000) #
# dim(all2000.duplicates) # 3336162
# it includes duplicated URLs: if one webpages includes multiple postcodes
# then it appears multiple times. This is ok for the nuts aggregation, but
# not for the frequencies
# This is only .co.uk

# one line for every host
all2000 <- unique(all2000.duplicates, by = c("host")) # 57897

# unique postcodes per website f table
f.websites.pc <- DescTools::Freq(all2000$V1, breaks = c(0, 1,2, 10,100,1000,10000,100000), ord = "desc")
f.websites.pc$level <- factor(f.websites.pc$level, levels = c("[0,1]","(1,2]", "(2,10]", "(10,100]",
                                                              "(100,1e+03]", "(1e+03,1e+04]",
                                                              "(1e+04,1e+05]"))
levels(f.websites.pc$level) <- c("(0,1]","(1,2]", "(2,10]", "(10,100]", "(100,1000]", "(1000,10000]", "(10000,100000]")

f.websites.pc <- f.websites.pc %>% 
  dplyr::select(-cumfreq) %>% 
  rename(Postcodes = level,
         F = freq,
         'F (%)' = perc,
         'Cummulative F' = cumperc)

kable(f.websites.pc,
      format = "latex",
      digits = 3,
      booktabs = T,
      format.args = list(big.mark = ","),
      caption = "Number of unique postcodes per .co.uk website, 2000.\\label{f2000}") %>%
  kableExtra::footnote(general = "Tranos et al. 2021",
                       general_title = "Source: ",
                       footnote_as_chunk = T)  
   # kable_classic(full_width = F, html_font = "Cambria")
```

Another data cleaning step dealt with some extreme outliers. Figure \ref{pc1} plots
the website counts for the top 1000 postcodes. Some obvious outliers can be observed 
for the 2004-2006 period, which can be attributed to a link farm [@BL2013links].  
The website counts for these six postcodes (SE24 9HP, CV8 2ED, GL16 7YA, CW1 6GL, 
M28 2SL, DE21 7BF), which in 2004 or 2005 had more than 1000 websites pointing to 
them, were replaced with predicted values based on a simple panel regression model 
with postcode fixed effects and yearly dummy variables. As the @sec-results illustrates,
this led to a small increase of the model predictive capability. To put the magnitude 
of the data imputation into perspective, this process affected 6 out of the 557,808
postcodes present in the data.

```{r echo=FALSE, message=FALSE, fig.cap="\\label{pc1}Yearly website counts per postcode (top 1000)", out.height="60%"}
path.image <- paste0(path, "/outputs/pc_year_1.png")
knitr::include_graphics(path.image)
```

To create the website density metric, the yearly website counts at the LAD level are
standardised by the number of firms in LAD to avoid biases associated with LAD 
hosting a large number of firms. Given that there is no such statistic for the OA,
the actual OA level counts are used. However, because of the the consistent spatial 
definition of OA (they host 40-250 households),[^4] website counts in OA are 
interpreted as a density metric too.

[^4]: [https://www.ons.gov.uk/methodology/geography/ukgeographies/statisticalgeographies](https://www.ons.gov.uk/methodology/geography/ukgeographies/statisticalgeographies)

<!-- system-level analysis -->

These website density metrics are used for the three different steps of the analysis. 
Firstly, a system-level analysis explores how the diffusion of web technologies in
the UK fits with the well-established S-curve. To do so, the following logistic 
function (Equation \ref{eq:s}) is estimated for the whole of the UK and for each 
LAD separately:

\begin{align}
y = k /(1 + e^{-(t-t_{o})})\label{eq:s}
\end{align}

\noindent $k$ is the asymptote or, in other words, the saturation level, $b$ the overall 
growth rate, and $t_{0}$ the *inflection point*
of maximum growth at $k/2$, where the logistic function is symmetrical [@wilson201281]. 
We compare  the LAD $t_0$ of each LAD against the $t_0$ of the UK to delineate 
whether a LAD reached that point faster or slower than the country average. Importantly, 
an accuracy criterion was imposed and only S-curves with $R^2 > 0.9$ were included 
in the analysis. To estimate Equation \ref{eq:s} a self-starting logistic growth 
model was employed using the `nls` and `SSlogis` functions in `R`.

<!-- **minimum R2 = 95%** in @wilson201281, see also @grubler1990rise -->
<!-- The literature usually uses the saturation level as the asymptote. I am -->
<!-- using the total number of websites as we cannot compute a rate. -->

The system-level analysis also focuses on the volatility of the web adoption to 
depict places with high concentration of early adopters and, equally, latecomers. 
To do so, the change of the ranking of the UK LAD over time is plotted and discussed. 
Both the S-curve and the volatility analysis focus only on LAD as the very large 
number of OA would have made such analysis diffucult to visualise and interpret.

<!-- ESDA -->

Next, exploratory spatial data analysis depicts whether the two main 
drivers of spatial diffusion -- namely neighbourhood and hierarchy -- underpin the
diffusion of web technologies in the UK. To capture the former and following 
@ding2010modeling the Moran's I and the Local Index of Spatial Association (LISA) 
are estimated for website density. To address the hierarchy mechanism, the Gini 
coefficient -- a well established metric of inequality -- is calculated. All the 
above are computed and plotted longitudinally both for the LAD and the OA.

<!-- RF -->

Lastly, a modelling framework is developed to test the above diffusion mechanisms.
The overarching aim is to build a model that can test the relationship between the
website density and these mechanisms: 

\begin{align}
Website\,Density_{t} \sim Hierarchy_{t-1} + Neighbourhood_{t-1} + S-curve_{t}\label{eq:rf.generic}
\end{align}

To estimate Equation \ref{eq:rf.generic}, Random Forest (RF) models were built. 
This is a popular machine learning algorith for both regression and classification 
problems [@biau2012analysis]. It was introduced by @breiman2001random and has  
gained popularity, becoming a go-to data science tool. RF can effectively 
handle skewed distributions and outliers, model non-linear relationships, require 
minimal hyperparameter tuning, exhibit low sensitivity to these parameters, and 
have relatively short training times [@Caruana2008; @liaw2002classification; @yan2020using]. 
These attributes match well with the website density data characteristics including 
skewness especially for OA. Also, the large data size (c. 230k data points for 
each of the 17 years) call for fast training times. 
Importantly, RF predictions tend to be more accurate than those from single regression 
trees and outperform Ordinary Least Squares in out-of-sample predictions, even with 
moderate-sized training data and a small number of predictors [@mullainathan2017machine; 
@athey2019machine; @sulaiman2011intelligent; @pourebrahim2019trip; @biau2012analysis].

RF is a tree-based ensemble learning algorithm [@breiman2001random]. It begins by 
generating random samples of the training data, which are then used to grow 
regression trees to predict the dependent variable. Data points and predictors are 
randomly sampled for the different trees. The trees are trained in parallel using their 
own bootstrapped samples of the training data. A crucial feature of RF is their 
ability not to overfit, meaning they can generalize well to unseen test data. While 
each tree may overfit individually, the ensemble of trees does not because the errors 
of individual trees are averaged, reducing the overall variance and preventing overfitting 
[@last2002improving]. For regression problems, RF predictions are made by averaging 
the predictions of all decision trees.

RF have been widely employed to address regression research problems. @pourebrahim2019trip 
combined a spatial interaction modeling framework with ML algorithms including RF
to predict commuting flows in New York City. @sinha2019assessing advocated for adopting
spatial ensemble learning approaches, such as RF, to model spatial data with high
autocorrelation and heterogeneity. @creditspatial predicted employment density in 
Los Angeles using spatially explicit RF. @guns2014recommending used RF to build a 
recommendation system for research collaborations. @ren2019predicting trained RF 
to predict the socio-economic status of cities using various online and mobility 
predictors. @tranos2023using utilised hyperlinks data and RF to make out-of-sample
predictions of interregional trade. @zhou2023geography employed such a framework
to assess whether key predictors of obesity differ across English cities.

# Results {#sec-results}

1.  S-shaped diffusion curves: S for LAD per firm. UK,
    fast/slow/examples

2.  ranks: there is stability and movement

3.  Neighbourhood effect: diffusion proceeds outwards from innovation
    centers, first "hitting" nearby rather than far-away locations
    (Grubler 1990) 

-   Moran's I: for OA and LAD over time

-   LISA maps: for OA and LAD over time More and less expected clusters.
    Different scales show different results

4.  Hierarchy effect: from main centers to secondary ones -- central
    places

-   Gini coefficient. Almost perfect polarisation of web adoption in the
    early stages at a granular level More equally diffused at the Local
    Authority level Plateau overtime

5.  RF

-   ideal: (i) train RF for all years and all (1) LAD and (2) OA with
    CAST and report metrics. (ii) train for all years and all but one
    region for (1) LAD and (2) OA to predict to the holdout region.
    Reports predictions as region similarities.

<!-- S-shaped -->

Figure \ref{s_uk} plots the S curve for the cumulative adoption of website technologies
in the UK during the 1996-2012 period. It demonstrates a pattern well aligned with
previous studies discussed in @sec-litreview. The vertical line for year 
2003 illustrates the point where the modeled cumulative adoption was equal to 50% 
of the maximum. This $t_0$ *inflection* point signals the maximum adoption speed 
and is used here to determine whether the UK LAD reached that inflection point 
earlier or later than the UK average. Specifically, the S curve and the inflection 
point are estimated for every LAD individually and then compared with the UK average. 
LAD that reached their inflection point earlier than the country average are labelled 
as *fast* and the rest as *slow*. Figure \ref{s_map} maps this pattern and the picture 
is not entirely clear. On the one hand, there are some expected examples of LAD
with relevant industrial backgrounds delineated as fast: the City of London, a 
world-renowned cluster of finance industries [@cook2007role], and Reading, a town 
with high-tech service industries in proximity to London and its main airport, Heathrow 
[@england2005polynet]. On the other hand, LAD which were expected to appear as fast
-- e.g. Hackney in central London and Bristol, a well-established creative cluster 
[@oatley1999cultural; @bassett2002cultural] -- were delineated as slow. Nevertheless, 
out of the 10 fastest LAD, eight were located in Greater London and one in 
Cambridge (see **Appendix**), but in overall there is a spatially heterogenous and
not easy to explain spatial pattern.

```{r echo=FALSE, message=FALSE, fig.cap="\\label{s_uk}Grwoth curve, UK"}
path.image <- paste0(path, "/outputs/s/s_uk_per_firm.png")
knitr::include_graphics(path.image)
```

```{r echo=FALSE, message=FALSE, fig.cap="\\label{s_map}LAD adoption rates"}
path.image <- paste0(path, "/outputs/s/speed_map.png")
knitr::include_graphics(path.image)
```

<!-- rankings -->
The next step is to assess the stability and volatility of the LAD in terms of 
their adoption of web technologies. As we know from the literature [@risk_perceptions],
different agents have different perceptions about and levels of acceptance of the 
risks and the potential economic returns associated with the adoption of new technologies 
-- see for instance the seminal work of @venkatesh2000theoretical. To reveal such 
aggregated patterns, Figures \ref{rank_stable} and \ref{rank_unstable} plot the ranking
of UK LAD based on website density. To decrease noise, the average ranking 
of 1996-1998 and 2010-2012 is plotted instead of the individual years. 
While there are quite a few obvious cases of LAD that maintained their position 
between the beginning and the end of the study period either at the top or at the 
bottom of the hierarchy (Figure \ref{rank_stable}), there are also 
quite a few LAD that changed drastically their position. Some of these LAD enjoyed
a process that at the first instance looks like leapfrogging since 
they managed to jump at the top of the hierarchy despite their slow start (top of 
Figure \ref{unstable}). There is extensive literature regarding the potential benefits of
technological leapfrogging. The underpinning argument is that latecomers can adopt
and benefit from new technologies that have been developed elsewhere without incurring 
the hefty initial R&D costs [@teece2008firm]. Although the leapfrogging literature
does not pay much attention to cities and regions [@yu2018sustainability], previous 
research highlighted the long term and sustained productivity benefit of the early
adopters of web technologies in the UK [@tranosuk]. So, although the LAD system 
is volatile, it is not clear whether the LAD with high concentration of late adopters 
will gain any latecomer benefits in a way similar to countries experiencing such
technological leapfrogging. 

```{r echo=FALSE, message=FALSE, fig.cap="\\label{rank_stable}Dynamics of wed diffusion: stability of LAD"}
path.image <- paste0(path, "/outputs/ranks/web_per_firm2000_2012_only05_av.png")
knitr::include_graphics(path.image)
```

```{r echo=FALSE, message=FALSE, fig.cap="\\label{rank_unstable}Dynamics of wed diffusion: volatility LAD"}
path.image <- paste0(path, "/outputs/ranks/web_per_firm2000_2012_only95_av.png")
knitr::include_graphics(path.image)
```

<!-- Neighbourhood effect -->

Then, figures \ref{morani} and \ref{lisa} offer a first insight into whether a neighbourhood 
effect underpins the diffusion of web technologies in the UK as they plot the Moran's I
and the LISA maps of website density respectively for LAD and OA. Starting from 
the former, spatial autocorrelation was higher in the beginning and then around 
year 2000 dropped slightly and stabled at 0.2. This reflects the early concentration 
of high website density around London, which over time diffused as high-high clusters
can be seen in other parts of the country away from London as seen in Figure \ref{lisa}. An 
almost reverse pattern can be observed for OA. At the beginning of the study period
Moran's I was around 0.1 and it plateaued after 2000-2001 around 0.2. Because of the 
very small size of OA, at the early stages of the diffusion of web technologies 
their adoption was spatially scattered. This is reflected in the lack of any significant
clusters in 1996. Eventually, as the adoption rate increased, more such clusters 
of high website density were formed and this is reflected both in the Moran's I 
and the LISA maps in Figures \ref{morani} and \ref{lisa}. All in all, the exploratory 
spatial data analysis advocates towards an underpinning neighbourhood effect and 
the different scales of analysis illustrate how it evolved differently over time. 

```{r, morani, echo=FALSE, message=FALSE, fig.cap="\\label{morani}Website density Moran's I"}

path.in.la <- paste0(path, "/outputs/lisa/corrected/LA/morani_la.csv")
path.in.oa <- paste0(path, "/outputs/lisa/corrected/oa/morani_oa.csv")

la.m <- read_csv(path.in.la) %>% 
  filter(p < 0.001) %>% # all values were significant
  mutate(spatial.unit = 'LAD')
oa.m <- read_csv(path.in.oa) %>% 
  filter(p < 0.001) %>% # all values were significant
  mutate(spatial.unit = 'OA')

bind_rows(la.m, oa.m) %>% 
  ggplot(aes(x=year, y=morani, fill= spatial.unit)) +
  geom_bar(stat = 'identity', position = 'dodge') +
  ylab('Moran\'s I') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_fill_manual('Spatial unit', 
                    values = c("LAD" = "grey80",
                                "OA"="grey48"),
                     limits = force) +
  scale_x_continuous(NULL, labels = as.character(1996:2012), breaks = 1996:2012) 
```

Figure \ref{lisa}

```{r, echo=FALSE, message=FALSE, fig.cap="\\label{lisa}\\centering Website density LISA maps", out.height="80%"}

path.image1 <- paste0(path, "/outputs/lisa/corrected/LA/lisa_level_pc1_la1996.png")
path.image2 <- paste0(path, "/outputs/lisa/corrected/LA/lisa_level_pc1_la2000.png")
path.image3 <- paste0(path, "/outputs/lisa/corrected/LA/lisa_level_pc1_la2012.png")
path.image4 <- paste0(path, "/outputs/lisa/corrected/OA/lisa_level_pc1_oa1996.png")
path.image5 <- paste0(path, "/outputs/lisa/corrected/OA/lisa_level_pc1_oa2000.png")
path.image6 <- paste0(path, "/outputs/lisa/corrected/OA/lisa_level_pc1_oa2012.png")
path.image7 <- paste0(path, "/outputs/lisa/corrected/LA/lisa_level_pc1_la_legend.png")

p1 <- ggdraw() + draw_image(path.image1)  
p2 <- ggdraw() + draw_image(path.image2)  
p3 <- ggdraw() + draw_image(path.image3)  
p4 <- ggdraw() + draw_image(path.image4) 
p5 <- ggdraw() + draw_image(path.image5)
p6 <- ggdraw() + draw_image(path.image6)  
p7 <- ggdraw() + draw_image(path.image7)  

par(mar=rep(0,4)) # no margins

library(patchwork)
# (p1 | p4)/(p2 | p5)/(p3 | p6)/ p7 +
#   #plot_layout(widths = 1)
#     plot_layout(widths = c(1, -0.5))

# p1 / plot_spacer() / p4 / p2 / plot_spacer() / p5 / p3 / plot_spacer() / p6 / p7 +
#   plot_layout(ncol = 3, widths = c(1, -0.85, 1), heights = c(1,1,1,2))

p <- p1 / plot_spacer() / p4 / p2 / plot_spacer() / p5 / p3 / plot_spacer() / p6 +
  plot_layout(ncol = 3, widths = c(1, -0.8, 1), heights = c(1,1,1))

# p + inset_element(p7, left = 0.45, bottom = -0.5, right = 0.6, top = 0.1)
p + inset_element(p7, left = 0.29, bottom = 0.8, right = 0.49, top = 1.2)

```

To illustrate whether a hierarchical process underpins the diffusion of web technologies,
the Gini coefficient is calculated yearly both for LAD and OA. As a metric of 
inequality, the Gini coefficient demonstrates whether website density is concentrated
in a small number of LAD/OA, or whether it is more equally spread across the country.
Both scales of analysis in Figure \ref{gini} illustrate the same picture. At the 
beginning of the commercial internet website density was extremely unequal, or in
other words, only a few places had websites associated with them. Inequality dropped 
and plateaued after 2000 for both scales illustrating that at the first stages of 
the commercial internet, website density was concentrated in a limited number of 
places. This is an indication of a hierarchical diffusion mechanism that led over 
time to a more equal spread. Interestingly, the year 2000 is again a period of change
for this diffusion mechanisms as it was for the neighbourhood process. 
There is a substantial difference between the Gini coefficient magnitude for LAD
and OA, but this is well expected as the very small size of OA equates to a lot 
polygons without any websites pointing to them -- for instance residential OA. 
LAD/OA

```{r, gini, echo=FALSE, message=FALSE, fig.cap="\\label{gini}Website density Gini coefficient"}

path.in.la <- paste0(path, "/outputs/lisa/gini_la.csv")
path.in.oa <- paste0(path, "/outputs/lisa/gini_oa.csv")

# path.in <- paste0(path, "/outputs/lisa/gini_la.csv")
# path.in <- paste0(path, "/outputs/lisa/gini_oa.csv")

la.gini <- read_csv(path.in.la) %>% 
  mutate(spatial.unit = 'LAD')
oa.gini <- read_csv(path.in.oa) %>% 
  mutate(spatial.unit = 'OA')

bind_rows(la.gini, oa.gini) %>%
  rename(Gini = 'gini(a$n)') %>% 
  ggplot(aes(x=year, y=Gini, fill= spatial.unit)) +
  geom_bar(stat = 'identity', position = 'dodge') +
  ylab('Gini') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_fill_manual('Spatial unit', 
                    values = c("LAD" = "grey80",
                                "OA"="grey48"),
                     limits = force) +
  scale_x_continuous(NULL, labels = as.character(1996:2012), breaks = 1996:2012) 
```

<!-- RF results -->

The next section incorporates the above discussed spatial processes of the
diffusion of web technologies into a modelling framework. The aim
is to use variables depicting these spatial processes in order to
predict the diffusion of web technologies in the UK over space and time
and across different scales. Specifically, four different models are
estimated. Firstly, all the data points for the OA and LAD are utilised
in order to build two respective RF models and assess their capacity to predict the
adoption of web technologies. These two models will reveal the
predictive capacity of the diffusion mechanisms and also allow to see how
the importance of such variables changes across scales. The next two
sets of models will be again trained on web diffusion at the two working scales:
OA and LAD. However, instead of using all the data points, the OA and
the LAD from one of the twelve UK regions are held out and then the trained
model is used to predict website density in the OA or the LAD of the
held-out region. This process takes place recursively for all UK
regions. The difference in the predictive capacity of the different
samples will reveal how dissimilar are these spatial process across
regions and, importantly, at different scales.

It needs to be highlighted here that the cross-validation for all models
is spatially and temporally sensitive. Instead of using
10 random folds, we employ the `CAST` package which allows holding back
data points from specific years and spatial units and use them for
testing in order to estimate the model performance
[@meyer2018improving].

The models include variables that capture the three processes
that the relevant literature and the descriptive analysis presented above highlighted. 
Namely, the models capture: (i) a hierarchy effect with diffusion running from main 
centres to secondary ones, (ii) a neighborhood effect according to which diffusion 
first hits nearby locations, and (iii) the rather canonical pattern of diffusion 
over time as reflected in the S-shaped pattern in the cumulative level of adoption.

To capture the hierarchy effect the models include as predictors a one
year lag of website density in London, the largest city in the UK, a
one year lag of the website density in the nearest city and the same for
the nearest retail centre. Due to the small sizes of the retail centres,
the latter is only relevant for the OA-level models. In addition, the
models include the distance to London, the nearest city and the nearest
retail centre. The underlying logic is that the level of website
adoption in a spatial unit depends on the level of the adoption in
places further up in the urban hierarchy the previous year. The inclusion of the
distance variables incorporates spatial structure into the hierarchy argument.
To depict the neighbourhood effect, the website density of the neighbouring spatial
units in the previous year is employed. Again, the underpinning rationale is that 
the level of web adoption within a spatial unit depends on the level of web adoption 
in the neighbouring spatial units the year before. This represents the 'hitting 
nearby locations first' argument. Therefore, the spatial and temporal lag of the 
website density in LAD and OA is calculated. Lastly, the time effect which is reflected 
on the S-curve for the cumulative adoption is captured by a time trend variable.
Hence, all four model will follow the following generic form 
(Eq. \ref{model}):

```{=tex}
\begin{align} \label{model}
Website\,Density_{t} \sim Distance\,London +
Website\,density\,London_{t-1} +\notag\\
Distance\,Nearest\,City +
Website\,density\,Nearest\,City_{t-1} +\notag\\
Distance\,Nearest\,Retail_{i} +
Website\,density\,Nearest\,Retail_{t-1} +\notag\\
W*\, Website\,density_{t-1} +\notag\\ 
year_{t}
\end{align}
```

To assess the predictive capability of the model, three broadly utilised
metrics are employed: the coefficient of determination ($R^2$), mean
absolute error (MAE) and root mean square error (RMSE):

```{=tex}
\begin{align}
R^2 = 1 - \frac{\sum_{k} (y_{k} - \hat{y_{k}})^2} {\sum_{k} (y_{k} - \overline{y_{k}})^2} \label{eq:rsquared}
\end{align}
```
```{=tex}
\begin{align}
MAE = \frac{1}{N} \sum_{k = 1}^{N} |\hat{y_{k}} - y_{k}| \label{eq:mae}
\end{align}
```
```{=tex}
\begin{align}
RMSE =  \sqrt{\frac{\sum_{k = 1}^{N} (\hat{y_{k}} - y_{k})^2} {N}} \label{eq:rmse}
\end{align}
```

$y_{k}$ is the $k^{th}$ observation of the dataset, which consists of
$N$ observations in total. $\hat{y_{k}}$ is the $k_{th}$ predicted value
for the dependent variable and $\overline{y_{k}}$ is the average value
of $y$. The last two metrics are expressed in the same units as the
dependent variable -- websites per firm for the LAD modes and the number
of websites for the OA models -- while the first one is the coefficient
of determination between the observed and the predicted values of
website adoption. Regarding $MAE$, it is the absolute difference between
the observed and the predicted website adoption. While $MAE$ does not
penalise for large errors, $RSME$ does so as it is proportional to the
squared difference between the observed and the predicted trade flows.
Hence larger errors weigh more for $RMSE$ [@pontius2008components].

@tbl-model-metrics presents the model performance for the
first set of models, for which all data points are employed for training
and testing via cross validation. The first one is trained and tested on 374 LAD
and the second on 232,296 OA, both for a 16 year period (1997-2012). The results are 
remarkably good considering that the are the outcome of space and time sensitive 
CV, so the the model does not suffer from overfitting. At the LAD level the model 
predicts 81% of the variation of website density. Both error metrics indicate that
the model error is $(\frac{1}{20}, \frac{1}{30})$ of a website per firm. At 
the OA the $R^2$ drops down to 21%. Considering its granularity, this is still 
a remarkable performance. To contextualise it, the model results in a MAE of one 
website for areas small enough to host less than 140 households.^[According to the Office for
National Statistics, 80% of OA in England and Wales host 110-139 households, [www.ons.gov.uk](https://www.ons.gov.uk/census/2001censusandearlier/dataandproducts/outputgeography/outputareas).] 
Because of the small size of the spatial units, the distribution is highly skewed
and a significant part of them is not linked to any websites. In 1997 only 1% of 
the UK OA were associated with at least one website. This should not come as a surprise as 
this was the very beginning of the commercial internet and any activities with a 
digital footprint were concentrated in a handful of areas. This was clearly illustrated
in Figure \ref{lisa}. At the end of the study period almost half of the UK OA were
not associated with a website. Again, given the granularity of the data this should
not come as a surprise. 

|                   | RMSE  | $R^{2}$   | MAE   |
|-------------------|-------|-----------|-------|
| Local Authorities | 0.032 | 0.810     | 0.019 |
| Output Areas      | 5.000 | 0.205     | 1.047 |

: Model metrics {#tbl-model-metrics}

Figures \ref{var.imp.LAD} and \ref{var.imp.OA} plot the importance of the different 
predictors. When the focus is on the LAD, the website density in the nearest city,
in London and in the neighbouring LAD the year before are the most important predictors.
They are followed by the yearly trend, while the spatial configuration as reflected
in distances to London or the nearest city only play a minor role. This can be attributed
to the rather coarse spatial scale of analysis. Nevertheless, all previously discussed
spatial processes are at play in the diffusion of web technologies at the LAD level: 
the first two predictors depict the hierarchical effect, the spatial and temporal lag of website
density depict the neighbourhood effect and the yearly trend the time-sensitive 
cumulative adoption pattern. 

When the much more granular scale of OA is adopted, the picture is reversed. The 
most important predictors are the three distance variables to London, the nearest 
city and the nearest retail centre. They stil depict the hierarchical effect, but
proximity to the different population centres is more important than their lagged 
web densities in predicting website diffusion. The neighbouring effect is less 
important at this scale. What is interesting is the almost negligible role of the 
yearly trend and London's website density. While the former probably illustrates 
the large heterogeneity in how web technologies have been adopted at this very fine 
scale, the later highlights that the importance of past web adoption rates in large 
population centres is surpassed by proximity to them and spatial configuration 
at this scale.

```{r varimpLA, eval = T, echo=FALSE, message = F, fig.cap="\\label{var.imp.LAD}Variable importance, LAD"}
path.image <- paste0(path, "/outputs/rf/figures/varimp_LA.png")
knitr::include_graphics(path.image)
```

```{r varimpOA, eval = T, echo=FALSE, message = F, fig.cap="\\label{var.imp.OA}Variable importance, OA"}
path.image <- paste0(path, "/outputs/rf/figures/varimp_OA.png")
knitr::include_graphics(path.image)
```

Figure \ref{var.imp} to replace Figures \ref{var.imp.LAD} and \ref{var.imp.OA} once
I have the variable importance for LAD.

```{r varimp, eval = T, echo=FALSE, message = F, fig.cap="\\label{var.imp}Variable importance"}
path.importance.lad <- paste0(path, "/outputs/rf/figures/varimp_LA_corrected.csv")
path.importance.oa <- paste0(path, "/outputs/rf/figures/varimp_OA_corrected.csv")

importance.oa <- read_csv(path.importance.oa)
importance.lad <- read_csv(path.importance.oa) %>% 
  mutate(importance = ifelse(variable == "year", 10, 
                             ifelse(variable == "distance to London", 75, importance)))

importance.oa %>% 
  rename(OA = importance) %>% 
  left_join(importance.lad %>% rename(LAD = importance), by = "variable") %>%
  pivot_longer(!variable, names_to = "spatial.unit", values_to = "importance") %>% 
  # reorder with fct_reorder
  ggplot(aes(x=importance, y=forcats::fct_reorder(variable, importance), 
         fill = spatial.unit)) +
  geom_col(position = 'dodge') +
  ylab('') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.x = element_text(vjust = 0.5, hjust=1)) +
  scale_fill_manual('Spatial unit', 
                    values = c("LAD" = "grey80",
                                "OA"="grey48"),
                     limits = force) +
  scale_x_continuous('Variable importance') 

```


Table \ref{table.regions} presents the results of the recursive hold out models,
which aim to highlight the potential regional heterogeneity of the spatial processes
behind the diffusion of web technologies. To begin with, as highlighted before, 
there is a difference of magnitude 
of one order between the LAD and the OA prediction errors, which is aligned with 
previous results that employed all data points. What is of interest here is the 
regional comparison. Table \ref{table.regions} illustrates some striking similarities, 
but also a few significant differences. The regions the web diffusion of which is 
better predicted using models trained in the rest of the country are the same 
despite the scale of analysis: South East, Wales, Yorkshire and The Humber and 
the North East of England. In other words, these are the regions whose spatial 
diffusion mechanisms of web technologies is closer to the country's average. Despite the 
consistency across scales, this is a diverse set of regions: **ADD CHARACTERISTICS**.

At the other end of the spectrum, Scotland's and the North West's web diffusion
mechanisms are consistently diverging from the country's average. This should not
come as a surprise as these regions are characterised of high levels of rurality
and remoteness. Similarly, London diffusion mechanisms diverge from the country's 
average and this is consistent across scales. London's uniqueness in UK's urban 
system and economy is also reflected in the spatial diffusion mechanisms of web 
technologies within its LAD and OA. It needs to be highlighted though that the 
difference between the $R^2$ of LAD and OA is more that an order of magnitude 
signaling how difficult is to predict diffusion at such a small spatial scale.
Northern Ireland is an interesting case. While it ranks at the bottom of the scale
when the models are trained and tested on LAD data, when the modelling adopts the
more granular OA scale, the spatial mechanisms that shape the web diffusion within
this region appear to be closer to the country's average. At this scale, proximity,
or lack of, relative to the rest of the country become less important and the 
internal to the region spatial structure predictors start playing a more
important role **CHECK NI OA model**.

```{r regions}
path.lad <- paste0(path, "/outputs/rf/figures/test_regions_LA.csv")
path.oa <- paste0(path, "/outputs/rf/figures/test_regions_OA.csv")

lad <- read.csv(path.lad) %>% 
  rename('RSquared LAD' = Rsquared)

oa <- read.csv(path.oa) %>% rename(Region = test.region,
                                   'RSquared OA' = Rsquared)

lad %>% left_join(oa) %>% 
  mutate('Rank OA' = rank(desc(`RSquared OA`)),
         'Rank LAD' = rank(desc(`RSquared LAD`))) %>% 
  relocate(5, .after = 2) %>% 
  arrange(`Rank LAD`) %>% 
  kable(caption= "Regional differences\\label{table.regions}", 
        digits=3,
        col.names = c("Region", "$R^2$ LAD", "Rank LAD", "$R^2$ OA", "Rank OA")) 
```

# Discussion and conclusions {#sec-conclusions}

contrary to results from future studies regarding social media
[@lengyel2020role], web technologies did not exclusively spread from a
central location.

<!-- reset the figure, table and section numbering -->
\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}

\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}

\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}

# Appendices {.appendix}

\counterwithin{figure}{section}
\counterwithin{table}{section}

## $t_0$ estunates for all LAD 

```{r, echo=FALSE, message = F, error=FALSE}

path.geo <- paste0(path, "/data/raw/Local_Authority_Districts_(December_2021)_UK_BUC.geojson")
la <- st_read(path.geo, quiet = T)
# source: https://geoportal.statistics.gov.uk/

# spatial transformations
la <- st_transform(la, 4326)

s.la.path <- paste0(path, "/data/temp/s_la_per_firm.csv")
read.csv(s.la.path) %>% 
  left_join(la %>% as_tibble() %>% select(LAD21CD, LAD21NM),
            by = c("ladcd" = "LAD21CD")) %>% 
  arrange(estimate) %>% 
  select(LAD21NM, estimate, std.error, r2, fast) %>% 
  rename(LAD = LAD21NM,
         "t_0 estimate" = "estimate",
         "R-squared" = "r2",
         "Std. error" = "std.error",
         "Diffusion speed" = "fast") %>% 
  kable(caption= "S-curve estiamtes for LAD\\label{table.s.lads}", 
        digits=3,
        align = c("l", "c", "c", "c", "c"),
        col.names = c("LAD", "$t_0$ estimate", "Std. error", "$R^2$", "Diffusion speed"))
```

# References {#references .unnumbered}
