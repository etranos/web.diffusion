---
title: "A multi-scale story of the diffusion of a new technology: the web"
author:
  - name: Emmanouil Tranos
    email: e.tranos@bristol.ac.uk
    affiliations: 
      - id: some-tech
        name: University of Bristol and The Alan Turing Institute
        department: 
        address: 
        city: 
        state: State
        postal-code: UK
    corresponding: e.tranos@bristol.ac.uk
address:
  - attributes:
        corresponding: true
    note: This is the first author footnote.
abstract: |
  This paper maps the participation in the digital economy and its evolution in the UK over space and time. Most of the existing economic geography literature which dealt with the spatiality of the internet employed supply-side measures, such as infrastructural capacity, in order to understand the geography of the digital economy and its potential spatial economic effects. Useful as these approaches might have been, they cannot capture the micro-processes and the characteristics of the individual online behaviour. Using large volumes of archived and geolocated web content, this paper models the diffusion of web technologies over space and time in the UK. The data and geolocation strategy allow to capture these processes at a very granular spatial scale. The modelling approach, which is based on simple spatial analytical methods and on the estimation of diffusion curves at various scales, enables to depict the role of geography and other cognitive factors which drove the diffusion of web technologies. Although the focus is on a recent historical period -- 1996-2012 -- the results of the analysis depict diffusion mechanisms which can be very useful in understanding the evolutionary patterns of the adoption of other newer technologies.
keywords: 
  - keyword1
  - keyword2
date: last-modified
bibliography: bibliography
format:
  elsevier-pdf:
    keep-tex: true
    journal:
      name: Journal Name
      formatting: preprint
      model: 3p
      cite-style: authoryear
editor: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE) # By default, hide code; set to TRUE to see code
#knitr::opts_chunk$set(fig.pos = 'p') # Places figures on their own pages
knitr::opts_chunk$set(out.width = '100%', dpi=300) # Figure resolution and size
knitr::opts_chunk$set(fig.env="figure") # Latex figure environment

#insert libraries here
library(rprojroot)
library(tidyverse)
library(knitr)
library(kableExtra)

# This is the project path
path <- find_rstudio_root_file()
```


# Introduction {#sec1}

<!-- geographers lack of interest + motivation -->

Geographers were always interested in how new technologies and
innovations diffuse across space and time and, importantly, how such
spatio-temporal processes can be modeled. After all, diffusion together
with invention and innovation are considered the pillars of
technological change [@das2022diffusion]. The seminal contribution of
@hagerstrand1968innovation is illustrative of this early interest.
However, the torch of exploring and modelling such processes had been
passed to other disciplines such as economics, business studies and
sociology well before the 'cultural turn' of economic geography
[@perkins2005international]. A potential explanation of the lack of
geographical studies exploring the diffusion of new and, more
specifically for this paper, digital technologies across both space
and time can be attributed to the scarcity of relevant and granular
enough data. As @zook2022mapping highlight, digital activities are
hardly ever captured in official data.

<!-- contribution -->

This paper offers such a contribution: a geographical study illustrating
how a new technology that is the web diffused over space and time in the
UK at a high level of spatial granularity during the $1996-2012$ period.
It does so by employing a novel source of big data which captures the
active engagement with web technologies during that period. By
addressing this empirical question this paper exemplifies how the
combination of data sources which escape the traditional social science
domain and adequate research methods can offer new lenses to
geographical research regarding the understanding of technological
diffusion.

**SAY SOMETHING ABOUT SPATIAL ANALYTICS, DIST PAPER BY ARP, NOT SPECIFIC EXPLANATORY VARIABLES, INTANGIBLE**

<!-- motivation -->

The motivation for this paper lies in the fact that there are various
stakeholders who are interested in knowing how new digital technologies
diffused over space and time and use this knowledge to make predictions
regarding the diffusion of related *future* technologies. As per
@leibowicz2016representing, historical studies agree on the fact that
"technologies diffuse at different times, at different rates, and to
different extents in different places, and can be significantly
influenced by policies" [@victor1993]. @meade2021modelling highlight
that a variety of actors have a direct interest in gaining such
knowledge including network equipment suppliers; network operators,
regulatory and local authorities. These processes and their effects vary
a lot across scales: although the diffusion of a new technology might
not be optimal at a local level, it might be beneficial from a global
perspective as it could lead to faster diffusion to less advantaged
places [@leibowicz2016representing]. Despite the spatial heterogeneity
of such diffusion mechanisms and the policy relevance, there are very
limited attempts in the literature to analyse the diffusion of new
digital technologies at a detailed geographical level.

<!-- diffusion and adoption -->

Technological diffusion, which is by definition an aggregated process,
can be discussed in parallel with individual adoption mechanisms. On the
one hand, @rogers2010diffusion identifies early adopter of new
technologies as 'knowledgeable risk takers' and @griliches1957 as
'profit maximisers' [@ding2010modeling]. Such individual agents are
rewarded because of their attitude towards new technologies and
innovations. On the other hand, @perkins2011internet attribute diffusion
to two processes: (i) epidemic-like mechanisms, which are governed by
distance, proximity and social interactions, and (ii) by economic
mechanisms as new innovations are adopted by users as they become more
profitable, valueable and useful.
<!-- The criticisms are mainly based on the fact that although the approach -->
<!-- gives an idea of aggregate (industry or household) behaviour, it does -->
<!-- not focus on the individual's (firm or household) adoption process -->

<!-- data and early engagement with the web -->

This paper focuses on the diffusion of the web as new technology during
the $1996-2012$ period. This was an exciting period for digital
technologies as it corresponds with the commercialisation of the
internet and, consequently, its almost universal adoption. The reader is
reminded that it was only in 1994 when Netscape Navigator was
introduced, a year before Microsoft's Internet Explorer.[^1] Also, only
$9$ per cent of UK's households had access to the internet in $1998$
[@ons2018], the web included mostly static webpages, there were no
social media and web browsing was happening exclusively from desktop PCs
as there were no smartphones [@tranosuk]. Hence, it is fair to say that
the study period captures the very early stages of the diffusion on a
new technology that is the web as well as its maturity. 
The former is a key point in the lifecycle of a
new technology. Firstly, during this period new technologies are
expensive, crude and imperfect [@rosenberg1994exploring; @wilson201281].
A simple comparison between Web 1.0 and Web 3.0 applications clearly
illustrates this [@tranos2020social]; for instance a static website compared 
with a platform like `github`, which enables cooperation between users and 
the creation of new information, meaning, and knowledge 
[@faraj2016special; @barassi2012does]. 
During this period the performance
of a new technology is the main attraction and not the cost to access
and use it [@wilson2011lessons]. There is a broader theoretical
discussion in the literature regarding the motivation behind early
adoption. As summarised by @perkins2005international, on the one hand,
epidemic models highlight the role of interpersonal contacts as a way
for new technologies to diffuse. On the other hand, economic models
underline the importance of heterogeneity. Different firms have
different structures and business plans, which define the potential
economic returns of the adoption of a new technology and, therefore, the
choice to adopt a new technology becomes an individual option. From a
broader and evolutionary perspective, initial conditions are essential
for the creation and evolution of path-dependent technological
development trajectories [@neffke2011regions; @simmie2014new]. This
argument is even more relevant when the focus is on digital technologies because 
of the commonly found lag between investment and economic returns as reflected in 
the Solow paradox [@acemoglu2014return; @brynjolfsson2018artificial].

[^1]: <https://www.theguardian.com/global/2015/mar/22/web-browser-came-back-haunt-microsoft>

Importantly, the data used here depicts the *active* engagement with the
web in the UK as it contains all commercial websites that 
(i) are part of the UK's relevant second level domain (SLD, .co.uk), 
(ii) have been archived by the Internet Archive[^2], and 
(iii) include a mention to at least one valid UK
postcode in the web text. 
The act of creating a website is understood here as active engagement with the
web vis-à-vis the more passive act of browsing the web or having an
internet connection [@tranosuk]. Previous studies have focused mostly on
more passive notions of engaging with digital technologies such as
internet adoption and internet speeds [e.g. @blank2018local;
@riddlesden2014broadband]. More details about the data and the data
generation process can be found in Section [XX](#sec3). 

<!-- description of the methods and results -->

<!-- contents -->

@grubler1990rise Later Hagerstrand conceptualized physical "barrier"
effects like lakes or uninhabited areas, which, in addition to distance,
act as further retarding effects on diffusion. These are formalized in
the form of "zero" or "half" contact multiplicators on the (distance
decaying) message flows.

@grubler1990rise With respect to the formalization of the communication
flows Hagerstrand defines a "mean information field" (MIF), in which the
probability of communication is a negative function of distance between
individuals

@wilson201281 Logistic growth describes an initial period of gradual
diffusion as a technology is introduced as a new commercial application,
moving then through a rapid, exponential growth phase, before slowing
and eventually saturating [@grubler1999dynamics]. The substitution of
incumbent technologies by new competitors leads to subsequent decline
and eventual obsolescence.

# Literature review {#sec2}

<!-- key concepts -- copied from help.Rmd -->

Geographical diffusion is a synthesis of different processes. On the one
hand, we can identify purely spatial or, in other words, contagious
processes. Adjacency and, more broadly, distance are the key drivers of
diffusion. This perspective draws similarities with epidemics:
innovation just like pathogens spreads because of contagion and,
consequently, proximity and exposure [@hivner2003facilitating]. On the
other hand, we can identify hierarchical processes. Instead of
horizontal distance-based diffusion mechanisms, the top-down hierarchy
of urban systems drives diffusion. In reality, the synthesis of these
two processes represents how new technologies diffuse over space and
time [@morrill2020spatial].

These ideas were firstly introduced by Torsten Hägerstrand and his
thesis entitled 'Innovation Diffusion as a Spatial Process'
[@hagerstrand1968innovation]. Hägerstrand was the first one to identify
diffusion as a geographical process. The starting point was the idea
that diffusion is based on passing information through social networks,
which themselves tend to be defined by geography. Hence, he identified
the 'neighbourhood' effect of how information, and consequently,
innovation diffuse. He used agricultural innovations to test and model
his ideas using Monte Carlo simulations. Hägerstrand also incorporated
the role of hierarchy and how some phenomena maybe firstly adopted in
larger cities and then diffuse to second tier ones. This is a sequential
instead of a simultaneous process, which resembles the 'lead-lag'
spatial acceleration effect in market research [@bento2018time;
@PERES201091]. Hägerstrand is more widely known though for highlighting
the role time plays in the diffusion of innovations: an early-pioneering
period, a middle fast accelerating period and a final saturation one
[@morrill2020spatial].

The temporal dimension was further explored by Everett Rogers and his
seminal work on 'Diffusion of Innovations' [@rogers2010diffusion].
Rogers being a sociologist, his work focused not on the diffusion of
innovations over space and time, but instead on the adoption of new
technologies and innovations by individuals and the individual
mechanisms that drive the decisions behind adoption. He identified five
groups of individuals regarding their adoption speed: innovators, early
adopters, early majority, late majority and laggards. The key mechanism of
diffusion and adoption is communication and how knowledge is
transferred within a social system. Therefore, all approaches agree on
the S-shaped diffusion and cumulative adoption pattern
[@grubler1990rise].

Schmidt's Law empirically illustrates a similar pattern. *Core* and
usually highly agglomerated regions is where new technologies are
invented and commercially deployed [@grubler1990rise].
This is where the first adopters tend to be based. Then, technologies
spread to the *rim* and eventually to the *periphery*. Although adoption
pace might be higher when new technologies finally arrive to the
periphery, the saturation levels there may never reach the ones in the
core because of the lack of infrastructure or other necessary
institutions [@leibowicz2016representing]. @grubler1990rise effectively
summarises the three key characteristics of the spatial diffusion
process: (i) the cumulative level of adoption follows an S-shaped
pattern just like purely temporal models; (ii) diffusion is shaped by a
hierarchy effect in a form of a centrifugal force: from core to
periphery; and (iii) diffusion is also shaped by distance and a
neighbourhood effect and contaminate nearby locations. These are the three
mechanisms that the empirical analysis in Section [4]{#sec4} investigates.

<!-- previous studies -->

The remaining of this section reviews empirical studies which analysed
the diffusion on new technologies over space and time. Although the
spatial dimension is present in most of the following studies, the level
of spatial detail is always more coarse than the one adopted
here. @beardsell1999spatial studied the evolution of the computer
industry in $317$ US metro areas during the $1977-1992$ period using
employment data. Their analysis indicated that the relative size
distribution holds for urban computer employment and also urban
heterogeneity is essential in explaining this distribution. In a recent
study, @bednarz2020pulled focused on wind turbines and modelled their
spatial diffusion across $402$ German regions during $1970-2015$. Their
key finding is that local demand than local supply was the main driving
factor.

At a global scale @perkins2005international explored whther the
diffusion rate of new technologies is driven by a latecomer advantage
and the engagement with the global economy via foreign direct
investments and trade. Their results illustrate that indeed latecomers
and developing countries experience diffusion of new technologies more
rapidly than early adopters and developed countries. At the same scale,
@perkins2011internet explored whether the adoption of previous
communication technologies that is mail, telegrams and telephones was
shaped by similar socioeconomic factors as the internet. Their results
indicated common patterns regarding the drivers behind the adoption of
different communication technologies.

Turning to studies that share more technological and scalar similarities
with this paper, @ding2010modeling modelled the spatial diffusion of
mobile telecommunications across regions in China. Their analysis
indicated that socioeconomic characteristics are important determinants
of the timing, speed and the level of mobile diffusion within China.
Using data from a Hungarian online social network, @lengyel2020role
analysed its adoption and the churn at a very granular spatial level.
Their results are in agreement with early theoretical and empirical
contributions reviewed here: assortativity, urban scaling and distance
are the key drivers of spatial diffusion. At a global scale
@PAPAGIANNIDIS2015308 modelled the diffusion of different web
technologies technologies and practices. Interestingly, they did so by
using similar, but less extensive data as the one used here. Their
analysis illustrated how the diffusion of different web technologies and
practices follow an S-shaped pattern as well as the different diffusion
rates of the different technologies and practices.

All in all, ...

<!-- @fritsch2015new analyze the spatial diffusion of laser technology -->
<!-- research in West Germany from 1960, when this technology began, until -->
<!-- 2005. -->
<!-- in one region  -->
<!-- @leibowicz2016representing for energy There is no generally accepted -->
<!-- theory that explains diffusion rate heterogeneity across technologies, -->
<!-- but several factors are considered important. Greater unit scale and -->
<!-- larger market size contribute to slower diffusion. Requirements for -->
<!-- interrelated technologies or complex infrastructures also hinder the -->
<!-- diffusion process (Grubler, 2012). -->

<!-- Mobile phones benefited from early deployment in recreational boats and -->
<!-- automobiles, where the traditional competitor was not a viable option. -->
<!-- In the early stages of diffusion, performance is a more important driver -->
<!-- of adoption than cost competitiveness. Typically, significant cost -->
<!-- reductions only occur once the technology reaches a deployment level -->
<!-- capable of supporting standardization and mass production (Wilson, -->
<!-- 2012). -->

<!-- @leibowicz2016representing Empirical evidence supports the validity of -->
<!-- Schmidt's Law over a wide range of technologies, time periods, and -->
<!-- geographical contexts. A recent meta-analysis of technology up-scaling -->
<!-- found that diffusion accelerated moving from the core to the rim and -->
<!-- periphery for technologies as diverse as natural gas power, oil -->
<!-- refineries, and automobiles (Wilson, 2009). One historical example that -->
<!-- conforms particularly well to Schmidt's Law is the diffusion of coal -->
<!-- power in Europe (Grubler, 2012). England emerged as the core region for -->
<!-- coal power because it had legal and economic institutions that -->
<!-- incentivized scientific pursuits, domestic coal reserves, and a clear -->

<!-- industrial motivation to replace water power with coal. ... -->

<!-- country level, not sure if needed. -->

<!-- @bento2018time explore What determines the duration of formative phases -->

<!-- for energy innovations in different markets? We are interested both in -->

<!-- initial markets (also: core, lead, first mover, early adopter) where -->

<!-- formative phases prepare technologies for mass commercialization, and in -->

<!-- follower markets (also: periphery, lag, late adopter) where accelerated -->

<!-- formative phases may benefit from diffusion and spillovers. -->

# Materials and Methods {#sec3}

**ADD FAMILY OF WEB TECHNOLOGIES**

To capture the diffusion of web technologies, a website density metric is developed 
for two different geographical scales: the Local Authority Districts (LAD) and the 
Output Areas (OA). The former is an administrative unit and there are 
\\textcolor{red}c. 380\\textcolor{black} such units in the UK. The latter is a census-based
geographical unit, which is very small as there are c. 230,000 of them in the UK.
This methodological choice will allow the mapping of the diffusion 
of web technologies and the assessment of the diffusion mechanisms 
at these two very different spatial scales.

The population of websites is calculated using data from the 
Internet Archive [^2] and, specifically, the JISC UK Web Domain Dataset [@ukwebarchive]. 
The Internet Archive is one of the most complete and oldest archive of webpages 
in the world operating since 1996 [@ainsworth2011much; @holzmann2016dawn].
It is a web crawler, which discovers webpages by following the hyperlinks
of every webpage its archives.
This dataset, which is curated by the British Library, contains all
the archived webpages from the UK ccTLD (.uk) from the 1996–2012 period. 
In essence, this is a long list of 2.5 billion URLs of archived webpages including 
also the archival timestamp. 

[^2]: [https://archive.org/](See%20https://archive.org/).

Instead of using the whole .uk country code top-level domain (ccTLD), this 
paper focuses on its commercial subset, the .co.uk SLD. 
This choice decrease the heterogeneity of the web data as 
such commercial websites have rather specific aims: they are used to diffuse information,
support online transactions and share opinions [@THELWALL2000441; @blazquez2018big]. 
Although a UK company can adopt a generic TLD such as.com and such cases escape 
the data used here, such omissions should not affect our results given the
popularity of the .uk ccTLD [@tranosuk]: UK consumers prefer to visit a .uk website
when they are searching for products or services [@hope]; and
anecdotal evidence indicates that during the first half of 2000, three
.co.uk domains were registered every minute [@oecd_coms]. 
Importantly, previous studies illustrated that .co.uk is the most popular UK SLD
[@tranosuk].

The text from these webpages was scanned using regular expressions (regex) to
identify strings of text which resemble UK postcodes and one fifth of them included
a mention to a postcode [@BL2013geo]. This information allows the geolocation of the 
data and the creation of the LAD and OA counts. 

Before that, the data 
cleaning process included a data aggregation step, through which
the webpages were aggregated to the relevant websites. 
Based on the following example, all three webpages listed below are part of the 
same website ([http://www.website.co.uk](http://www.website.co.uk)) and at the end 
only websites and not the nested webpages were considered as otherwise the metric 
would have been biased towards large websites. 

  - [http://www.website.co.uk/webpage_a](http://www.website.co.uk/webpage_a) B152TT

  - [http://www.website.co.uk/webpage_b](http://www.website.co.uk/webpage_b) BS81TH

  - [http://www.website.co.uk/webpage_c](http://www.website.co.uk/webpage_c) B152TT

\noindent What is challenging is that this aggregation approach,
which has been used elsewhere [@tranosuk; @shoreditch] may lead to websites with 
multiple postcodes. As per the above example, ([http://www.website.co.uk](http://www.website.co.uk))
includes two unique postcodes: B152TT and BS81SS. 
The distribution of postcodes for 2000 is presented in Table \ref{f2000}, which 
clearly illustrates the wide range. On one hand, we
have websites anchored to a unique location (72% of all
the reconstructed websites), which may represent
a small company with a single trading location. On the other hand,
we have websites with thousands of different postcodes.
Considering the timeperiod of the analysis, such cases can represent directories
which used to be popular in the pre-search engines early times of internet [@tranosuk].[^3]

The analysis presented here is based on two subsets of these data.
Firstly, the analysis is based on websites which only contain only one unique postcode
and, therefore, the geolocation process might suffer from less noise. 
As a robustness check, the analysis is replicated the analysis for an extended subset 
of websites, which include up to ten unique postcodes. These websites are geolocated 
by equally attaching them to multiple locations. This extended sample includes 
94 percent of all the archived websites in 2000.

[^3]: See Figure A1 in **Appendix A in the supplemental data online for examples**.

```{r eval=T, echo=FALSE, results='asis'}

# load 2000 co.uk
# path.2000 <- "C:/Users/nw19521/OneDrive - University of Bristol/projects/archive/nuts/all2000couk.csv"
# path.2000 <- "/Users/nw19521/Library/CloudStorage/OneDrive-UniversityofBristol/projects/archive/nuts/all2000couk.csv"
path.2000 <- paste0(path, "/data/temp/all2000couk.csv")

all2000.duplicates <- data.table::fread(path.2000) #
# dim(all2000.duplicates) # 3336162
# it includes duplicated URLs: if one webpages includes multiple postcodes
# then it appears multiple times. This is ok for the nuts aggregation, but
# not for the frequencies
# This is only .co.uk

# one line for every host
all2000 <- unique(all2000.duplicates, by = c("host")) # 57897

# unique postcodes per website f table
f.websites.pc <- DescTools::Freq(all2000$V1, breaks = c(0, 1,2, 10,100,1000,10000,100000), ord = "desc")
f.websites.pc$level <- factor(f.websites.pc$level, levels = c("[0,1]","(1,2]", "(2,10]", "(10,100]",
                                                              "(100,1e+03]", "(1e+03,1e+04]",
                                                              "(1e+04,1e+05]"))
levels(f.websites.pc$level) <- c("(0,1]","(1,2]", "(2,10]", "(10,100]", "(100,1000]", "(1000,10000]", "(10000,100000]")

f.websites.pc <- f.websites.pc %>% 
  dplyr::select(-cumfreq) %>% 
  rename(Postcodes = level,
         F = freq,
         'F (%)' = perc,
         'Cummulative F' = cumperc)

kable(f.websites.pc,
      format = "latex",
      digits = 3,
      booktabs = T,
      format.args = list(big.mark = ","),
      caption = "Number of unique postcodes per .co.uk website, 2000.\\label{f2000}") %>%
  kableExtra::footnote(general = "Tranos et al. 2021",
                       general_title = "Source: ",
                       footnote_as_chunk = T)  
   # kable_classic(full_width = F, html_font = "Cambria")
```

To create the web density metric, the yearly website counts at the LAD level are
standardised by the number of firms in LAD to avoid biases associated with LADs 
hosting a large number of firms. Given that there is no such statistic for the OA,
the actual OA level counts are used. However, because of the the rather
consistent spatial definition of OA (they host 40-250 households),[^4] 
website counts in OA are interpreted as a density metric too.

[^4]: [https://www.ons.gov.uk/methodology/geography/ukgeographies/statisticalgeographies](https://www.ons.gov.uk/methodology/geography/ukgeographies/statisticalgeographies)

This website metric...

@wilson201281 growth function description in p. 86

from R: Asym/(1+exp((xmid-input)/scal)) using the terms from Wilson
k/(1 + e\^(t_0 - t)/scal)

Wilson: k(1 + e\^-b(t - t_0)) So, scal = - 1/b

The curve is symmetric\* around t_0

**minimum R2 = 95%**, see also @grubler1990rise

The literature usually uses the saturation level as the asymptote. I am
using the total number of websites as we cannot compute a rate.

Moran's I as @ding2010modeling **TODO** for t_0, diffusion speed

# Results {#sec4}

1.  S-shaped diffusion curves: S for LADs per firm. UK,
    fast/slow/examples

2.  ranks: there is stability and movement

3.  Neighbourhood effect: diffusion proceeds outwards from innovation
    centers, first "hitting" nearby rather than far-away locations
    (Grubler 1990) 

-   Moran's I: for OA and LAD over time

-   LISA maps: for OA and LAD over time More and less expected clusters.
    Different scales show different results

4.  Hierarchy effect: from main centers to secondary ones -- central
    places

-   Gini coefficient. Almost perfect polarisation of web adoption in the
    early stages at a granular level More equally diffused at the Local
    Authority level Plateau overtime

5.  RF

-   ideal: (i) train RF for all years and all (1) LADs and (2) OAs with
    CAST and report metrics. (ii) train for all years and all but one
    region for (1) LADs and (2) OAs to predict to the holdout region.
    Reports predictions as region similarities.

### RF results

The next section incorporates the above discussed spatial processes of the
diffusion of web technologies into the same modelling framework. The aim
is to use variables depicting these spatial processes in order to
predict the diffusion of web technologies in the UK over space and time
and across different scales. Specifically, four different models are
estimated. Firstly, all the data points at the OA and LAD are utilised
in order to build RF models and assess their capacity to predict the
adoption of web technologies. These two models will reveal the
predictive capacity of the spatial processes and also allow to see how
the importance of such variables changes across scales. The next two
sets of models will be again trained on web diffusion at the two working scales:
OA and LAD. However, instead of using all the data points, the OA and
the LAD from one of the twelve UK regions are held out and then the trained
model is used to predict web adoption in the OA or the LAD of the
held-out region. This process takes place recursively for all UK
regions. The difference in the predictive capacity of the different
samples will reveal how dissimilar are these spatial process across
regions and, importantly, at different scales.

It needs to be highlighted here that the cross-validation for all models
is spatially and temporally sensitive. Instead of using
10 random folds, we employ the `CAST` package which allows holding back
data points from specific years and spatial units and use them for
testing in order to estimate the model performance
[@meyer2018improving].

The models need to include variables that capture the three processes
that the relevant literature and the descriptive analysis presented in
the Section **XX** highlighted. Namely, the models capture: (i) a
hierarchy effect with diffusion running from main centres to secondary
ones, (ii) a neighborhood effect according to which diffusion first hits
nearby locations, and (iii) the rather canonical pattern of 
diffusion over time as reflected in the S-shaped pattern in the
cumulative level of adoption.

To capture the hierarchy effect the models include as predictors a one
year lag of website density in London -- the largest city in the UK, a
one year lag of the website density in the nearest city and the same for
the nearest retail centre. Due to the small sizes of the retail centres,
the latter is only relevant for the OA-level models. In addition, the
models include the distance to London, the nearest city and the nearest
retail centre. The underlying logic is that the level of website
adoption in a spatial unit depends on the level of the adoption in
places further up in the urban hierarchy the previous year. To depict
the neighbourhood effect, the web density of the neighbouring spatial
units in the previous year is employed. Again, the underpinning
rationale is that the level of web adoption within a spatial unit
depends on the level of web adoption in the neighbouring spatial units
the year before. This is the 'hitting nearby locations first' argument.
Therefore, the spatial and temporal lag of the website density in LAD
and OA is calculated. Lastly, the time effect which is reflected on the
S curve for the cumulative adoption is captured by trend variable.
Hence, all four model will follow the following generic form 
(Eq. \ref{model}):

```{=tex}
\begin{align} \label{model}
Website\,Density_{t} \sim Distance\,London +
Website\,density\,London_{t-1} +\notag\\
Distance\,Nearest\,City +
Website\,density\,Nearest\,City_{t-1} +\notag\\
Distance\,Nearest\,Retail_{i} +
Website\,density\,Nearest\,Retail_{t-1} +\notag\\
W*\, Website\,density_{t-1} +\notag\\ 
year_{t}
\end{align}
```

To assess the predictive capability of the model, three broadly utilised
metrics are employed: the coefficient of determination (R squared), mean
absolute error (MAE) and root mean square error (RMSE):

```{=tex}
\begin{align}
R^2 = 1 - \frac{\sum_{k} (y_{k} - \hat{y_{k}})^2} {\sum_{k} (y_{k} - \overline{y_{k}})^2} \label{eq:rsquared}
\end{align}
```
```{=tex}
\begin{align}
MAE = \frac{1}{N} \sum_{k = 1}^{N} |\hat{y_{k}} - y_{k}| \label{eq:mae}
\end{align}
```
```{=tex}
\begin{align}
RMSE =  \sqrt{\frac{\sum_{k = 1}^{N} (\hat{y_{k}} - y_{k})^2} {N}} \label{eq:rmse}
\end{align}
```

$y_{k}$ is the $k^{th}$ observation of the dataset, which consists of
$N$ observations in total. $\hat{y_{k}}$ is the $k_{th}$ predicted value
for the dependent variable and $\overline{y_{k}}$ is the average value
of $y$. The last two metrics are expressed in the same units as the
dependent variable -- websites per firm for the LAD modes and the number
of websites for the OA models -- while the first one is the coefficient
of determination between the observed and the predicted values of
website adoption. Regarding $MAE$, it is the absolute difference between
the observed and the predicted website adoption. While $MAE$ does not
penalise for large errors, $RSME$ does so as it is proportional to the
squared difference between the observed and the predicted trade flows.
Hence larger errors weigh more for $RMSE$ [@pontius2008components].

Table \ref{table.metrics.all} presents the model performance for the
first set on models, for which all data points are employed for training
and testing via cross validation. The first one is trained and tested on 374 LAD
and the second on 232,296 OA, both for a 16 year period (1997-2012). The results are 
remarkably good considering that the are the outcome of space and time sensitive 
CV, so the the model does not suffer from overfitting. At the LAD level the model 
predicts 81% of the variation of website density. Both error metrics indicate that
the model error is $(\frac{1}{20}, \frac{1}{30})$ of a website per firm. At 
the OA the RSquared drops down to 21%. Considering its granularity, this is still 
a remarkable performance. To contextualise it, the model results in a MAE of one 
website for areas small enough to host less than 140 households.^[According to the Office for
National Statistics, 80% of OA in England and Wales host 110-139 households, [www.ons.gov.uk](https://www.ons.gov.uk/census/2001censusandearlier/dataandproducts/outputgeography/outputareas).] 
Because of the small size of the spatial units, the distribution is highly skewed
and a significant part of them is not linked to any websites. In 1997 only 1% of 
the UK OA were associated with a website. This should not come as a surprise as 
this was the very beginning of the commercial internet and any activities with a 
digital footprint were concentrated in a handful of areas. This was clearly illustrated
in Section **ADD**. At the end of the study period almost half of the UK OA were
not associated with a website. Again, given the granularity of the data this should
not come as a surprise. 

|                   | RMSE  | RSquared | MAE   |
|-------------------|-------|----------|-------|
| Local Authorities | 0.032 | 0.810    | 0.019 |
| Output Areas      | 5.000 | 0.205    | 1.047 |

: Model metrics {#table.metrics.all}

Figures \ref{var.imp.LAD} and \ref{var.imp.OA} plot the importance of the different 
predictors. When the focus is on the LAD, the website density in the nearest city,
in London and in the neighbouring LADs the year before are the most important predictors.
They are followed by the yearly trend, while the spatial configuration as reflected
in distances to London or the nearest city only play a minor role. This can be attributed
to the rather coarse spatial scale of analysis. Nevertheless, all previously discussed
spatial processes are at play in the diffusion of web technologies at the LAD level: 
the first two predictors depict the hierarchical effect, the spatial and temporal lag of website
density depict the neighbourhood effect and the yearly trend the time-sensitive 
cumulative adoption pattern. 

When the much more granular scale of OA is adopted, the picture is reversed. The 
most important predictors are the three distance variables to London, the nearest 
city and the nearest retail centre. They stil depict the hierarchical effect, but
proximity to the different population centres is more important than their lagged 
web densities in predicting website diffusion. The neighbouring effect is less 
important at this scale. What is interesting is the almost negligible role of the 
yearly trend and London's website density. While the former probably illustrates 
the large heterogeneity in how web technologies have been adopted at this very fine 
scale, the later highlights that the importance of past web adoption rates in large 
population centres is surpassed by proximity to them and spatial configuration 
at this scale.

```{r varimpLA, eval = T, echo=FALSE, message = F, fig.cap="\\label{var.imp.LAD}Variable importance, LAD"}
path.image <- paste0(path, "/outputs/rf/figures/varimp_LA.png")
knitr::include_graphics(path.image)
```

```{r varimpOA, eval = T, echo=FALSE, message = F, fig.cap="\\label{var.imp.OA}Variable importance, OA"}
path.image <- paste0(path, "/outputs/rf/figures/varimp_OA.png")
knitr::include_graphics(path.image)
```

Table \ref{table.regions} presents the results of the recursive hold out models,
which aim to highlight the potential regional heterogeneity of the spatial processes
behind the diffusion of web technologies. To begin with, as highlighted before, 
there is a difference of magnitude 
of one order between the LAD and the OA prediction errors, which is aligned with 
previous results that employed all data points. What is of interest here is the 
regional comparison. Table \ref{table.regions} illustrates some striking similarities, 
but also a few significant differences. The regions the web diffusion of which is 
better predicted using models trained in the rest of the country are the same 
despite the scale of analysis: South East, Wales, Yorkshire and The Humber and 
the North East of England. In other words, these are the regions whose spatial 
diffusion mechanisms of web technologies is closer to the country's average. Despite the 
consistency across scales, this is a diverse set of regions: **ADD CHARACTERISTICS**.

At the other end of the spectrum, Scotland's and the North West's web diffusion
mechanisms are consistently diverging from the country's average. This should not
come as a surprise as these regions are characterised of high levels of rurality
and remoteness. Similarly, London diffusion mechanisms diverge from the country's 
average and this is consistent across scales. London's uniqueness in UK's urban 
system and economy is also reflected in the spatial diffusion mechanisms of web 
technologies within its LAD and OA. It needs to be highlighted though that the 
difference between the RSquared of LAD and OA is more that an order of magnitude 
signaling how difficult is to predict diffusion at such a small spatial scale.
Northern Ireland is an interesting case. While it ranks at the bottom of the scale
when the models are trained and tested on LAD data, when the modelling adopts the
more granular OA scale, the spatial mechanisms that shape the web diffusion within
this region appear to be closer to the country's average. At this scale, proximity,
or lack of, relative to the rest of the country become less important and the 
internal to the region spatial structure predictors start playing a more
important role **CHECK NI OA model**.

```{r regions}
path.lad <- paste0(path, "/outputs/rf/figures/test_regions_LA.csv")
path.oa <- paste0(path, "/outputs/rf/figures/test_regions_OA.csv")

lad <- read.csv(path.lad) %>% 
  rename('RSquared LAD' = Rsquared)

oa <- read.csv(path.oa) %>% rename(Region = test.region,
                                   'RSquared OA' = Rsquared)

lad %>% left_join(oa) %>% 
  mutate('Rank OA' = rank(desc(`RSquared OA`)),
         'Rank LAD' = rank(desc(`RSquared LAD`))) %>% 
  relocate(5, .after = 2) %>% 
  arrange(`Rank LAD`) %>% 
  kable(caption= "Regional differences\\label{table.regions}", digits=3)
```

# Discussion and conclusions 

contrary to results from future studies regarding social media
[@lengyel2020role], web technologies did not exclusively spread from a
central location.

# References {#references .unnumbered}
